{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgekvhAkD6MJ"
      },
      "source": [
        "# 阶段0: 环境检测与导入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P68Wz1FQ9cr3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from datasets import load_dataset\n",
        "import sentencepiece as spm\n",
        "import tempfile\n",
        "from datetime import timedelta\n",
        "from dataclasses import dataclass\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Sampler\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91lzPObXD_Rc",
        "outputId": "e1ec52ac-ee6d-43b3-e9d0-5a2d647e9e6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
            "GPU Memory: 8.59 GB\n"
          ]
        }
      ],
      "source": [
        "# 检查GPU是否可用\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "  print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"No GPU available, using CPU\")\n",
        "\n",
        "# 设置随机种子, 保证可以复现\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OahhQXkfQnt"
      },
      "source": [
        "# 阶段1: Embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTsx19avr_Ed"
      },
      "source": [
        "## 1.1 Token Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sbBloQd7gesR"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, vocab_size:int, d_model:int):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.lut = nn.Embedding(vocab_size, d_model) # W_E: (vocab_szie, d_model)\n",
        "    # X @ W_E 不合法是因为这里不是在做矩阵乘法\n",
        "    # Y[b, t, :] = W_E(X[b, t, :]) 实际上时在索引\n",
        "\n",
        "  def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    x: (batch_size, seq_len)\n",
        "    return: (batch_size, seq_len, d_model)\n",
        "    \"\"\"\n",
        "    return self.lut(x) * math.sqrt(self.d_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_rMMhgOsEi8"
      },
      "source": [
        "## 1.2 Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-wz80BllsJqx"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model:int, dropout:float=0.0, max_len:int=5000):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    # position: (max_len, 1)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "    # div_term: (1, d_model/2)\n",
        "    div_term = torch.exp(\n",
        "        torch.arange(0, d_model, 2, dtype=torch.float)\n",
        "        * (-math.log(10000) / d_model)\n",
        "    ).unsqueeze(0)\n",
        "\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    pe = pe.unsqueeze(0) # ->(1, max_len, d_model) 方便与batch相加(广播机制)\n",
        "\n",
        "    # 注册为buffer\n",
        "    self.register_buffer(\"pe\", pe)\n",
        "\n",
        "  def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "    x = x + self.pe[:, :x.size(1)] # x; (batch_size, seq_len, d_modle) 即上面embed的输出\n",
        "    return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXl-YYGi7m4C"
      },
      "source": [
        "# 阶段2: Attention机制"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJNmG1tH8Dmq"
      },
      "source": [
        "## 1.1 Scaled Dot-Product Attention\n",
        "$$\n",
        "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dArtV08H7uXa"
      },
      "outputs": [],
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "  \"\"\"\n",
        "  计算缩放点积 Scaled Dot-Product Attention\n",
        "  query: (batch, headi, seq_len_q, d_k)\n",
        "  key:   (batch, headi, seq_len_k, d_k)\n",
        "  value: (batch, headi, seq_len_k, d_v)\n",
        "  mask:  (batch, 1 or headi, seq_len_q, seq_len_k) padding mask or subsequent mask\n",
        "  output: (batch, h, seq_len_q, seq_len_k)\n",
        "  \"\"\"\n",
        "  d_k = query.size(-1)\n",
        "\n",
        "  # 1. QK^T / sqrt(kd)\n",
        "  scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)# scores: (batch, h, seq_q, seq_k)\n",
        "\n",
        "  # 2. mask(被mask的位置为-inf)\n",
        "  if mask is not None:\n",
        "    mask = mask.to(torch.bool)\n",
        "    scores = scores.masked_fill(~mask, float('-inf'))\n",
        "\n",
        "\n",
        "  # 3. softmax\n",
        "  attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "  # 4.dropout\n",
        "  if dropout is not None:\n",
        "    attention_weights = dropout(attention_weights)\n",
        "\n",
        "  # V的加权求和\n",
        "  output = torch.matmul(attention_weights, value)\n",
        "\n",
        "  return output, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR_QwjQfGJk6"
      },
      "source": [
        "## 2.2 Multi-Head Attention\n",
        "\n",
        "$$\n",
        "\\mathrm{MultiHead}(Q, K, V) =\n",
        "    \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O \\\\\n",
        "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
        "$$\n",
        "\n",
        "Where the projections are parameter matrices $W^Q_i \\in\n",
        "\\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in\n",
        "\\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in\n",
        "\\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in\n",
        "\\mathbb{R}^{hd_v \\times d_{\\text{model}}}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gzLgIVvyGtBs"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, dropout=0.0):\n",
        "    super().__init__()\n",
        "    assert d_model % num_heads == 0\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.h = num_heads\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    # 四个Linear networks, 每个的大小是(512, 8*64)的\n",
        "    self.W_q = nn.Linear(d_model, d_model)\n",
        "    self.W_k = nn.Linear(d_model, d_model)\n",
        "    self.W_v = nn.Linear(d_model, d_model)\n",
        "    self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, query, key, value, mask=None):\n",
        "    batch_size, seq_len_q, _ = query.size()\n",
        "    seq_len_k = key.size(1)\n",
        "\n",
        "    # 我们假设d_k == d_v\n",
        "    assert key.size(1) == value.size(1)\n",
        "\n",
        "    # 1. 线性映射\n",
        "    # x;(batch seq_len, d_model) @ W:(d_model, d_model) -> (batch seq_len, d_model)\n",
        "    Q = self.W_q(query)\n",
        "    K = self.W_k(key)\n",
        "    V = self.W_v(value)\n",
        "\n",
        "    #2. 分头\n",
        "    Q = Q.view(batch_size, seq_len_q, self.h, self.d_k).transpose(1, 2)\n",
        "    K = K.view(batch_size, seq_len_k, self.h, self.d_k).transpose(1, 2)\n",
        "    V = V.view(batch_size, seq_len_k, self.h, self.d_k).transpose(1, 2) # [batch, h, seq_len, d_k]\n",
        "\n",
        "    # 3. 计算注意力得分\n",
        "    out, attn = attention(Q, K, V, mask=mask, dropout=self.dropout)\n",
        "\n",
        "    # 4. 合并\n",
        "    out = out.transpose(1, 2).contiguous()\n",
        "    out = out.view(batch_size, seq_len_q, -1) # [batch, seq_len, d_model]\n",
        "\n",
        "    # 5. 投影\n",
        "    out = self.W_o(out)\n",
        "\n",
        "    return out, attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5VlQJvUZFPR"
      },
      "source": [
        "## 2.3 mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zCtpBoiHaCBk"
      },
      "outputs": [],
      "source": [
        "def make_src_mask(src, pad_idx=0):\n",
        "  \"\"\"\n",
        "  encoder padding mask\n",
        "  src: (batch, src_seq_len)\n",
        "  \"\"\"\n",
        "  batch_size, seq_len = src.size()\n",
        "  mask = (src != pad_idx).unsqueeze(1).unsqueeze(2) # mask: (batch, 1, 1, src_seq_len)\n",
        "  mask = mask.expand(batch_size, 1, seq_len, seq_len).contiguous()\n",
        "  return mask\n",
        "\n",
        "def make_tgt_mask(tgt, pad_idx=0):\n",
        "  \"\"\"\n",
        "  decoder padding + subsequent mask\n",
        "  tgt: (batch, tgt_seq_len)\n",
        "  \"\"\"\n",
        "  batch_size, seq_len = tgt.size()\n",
        "\n",
        "  # padding mask\n",
        "  padding_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(2) # (batch, 1, 1, tgt_seq_len)\n",
        "  # subsequent mask\n",
        "  subsequent_mask = torch.tril(\n",
        "      torch.ones((seq_len, seq_len), device=tgt.device, dtype=torch.bool)\n",
        "  ).unsqueeze(0).unsqueeze(0) # (1, 1, tgt_seq_len, tgt_seq_len)\n",
        "\n",
        "  mask = (padding_mask & subsequent_mask).bool() # (batch, 1, seq_len ,seq_len)\n",
        "\n",
        "  return mask\n",
        "\n",
        "\n",
        "def make_cross_attn_mask(src, tgt, pad_idx=0):\n",
        "  \"\"\"\n",
        "  Decoder cross-attention mask\n",
        "  src: (batch, src_seq_len)\n",
        "  tgt: (batch, tgt_seq_len)\n",
        "  return: (batch, 1, tgt_seq_len, src_seq_len)\n",
        "  \"\"\"\n",
        "  batch_size = src.size(0)\n",
        "  src_len = src.size(1)\n",
        "  tgt_len = tgt.size(1)\n",
        "\n",
        "  # Src padding mask: (batch, 1, 1, src_len)\n",
        "  src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "  # 扩展到 (batch, 1, tgt_len, src_len)\n",
        "  mask = src_mask.expand(batch_size, 1, tgt_len, src_len)\n",
        "  return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Batch:\n",
        "    # 处理tgt, tgt_y 并计算ntoken\n",
        "    def __init__(self, src, tgt_input, tgt_y, pad_idx, make_src_mask_fn, make_tgt_mask_fn, make_cross_attn_mask_fn):\n",
        "        self.src = src\n",
        "        self.src_mask = make_src_mask_fn(src, pad_idx)\n",
        "        self.tgt = tgt_input      # [<bos>, y1, y2, y3]\n",
        "        self.tgt_y = tgt_y     # [y1, y2, y3, <eos>]\n",
        "\n",
        "        self.tgt_mask = make_tgt_mask_fn(self.tgt, pad_idx)\n",
        "        self.cross_attn_mask = make_cross_attn_mask_fn(src, self.tgt, pad_idx)\n",
        "        self.ntokens = (self.tgt_y != pad_idx).sum().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvnhmspwVBTL"
      },
      "source": [
        "# 阶段3: Position-wise FFN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiGYA3xLU_fC"
      },
      "source": [
        "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xUyIsdIVVPDz"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff=2048, dropout=0.0):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(d_model, d_ff)\n",
        "    self.linear2 = nn.Linear(d_ff, d_model)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "  \"\"\"\n",
        "  x = W1x + b1\n",
        "  x = ReLu(x)\n",
        "  x = dropout(x)\n",
        "  x = W2x + b2\n",
        "  \"\"\"\n",
        "  def forward(self, x):\n",
        "    return self.linear2(\n",
        "        self.dropout(\n",
        "            F.relu(self.linear1(x))\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-CqmLTJr-r0"
      },
      "source": [
        "# 阶段4: Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evcHY5feAdrH"
      },
      "source": [
        "## 4.1 Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MRE4FM6UuYcF"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, features, eps=1e-6): # features == d_model\n",
        "    super().__init__()\n",
        "    self.a_2 = nn.Parameter(torch.ones(features))\n",
        "    self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepdim=True)\n",
        "    std = x.std(-1, keepdim=True)\n",
        "    return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O83OXBPXvOEW"
      },
      "outputs": [],
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "  # 层归一化 + 残差连接 Add&Norm\n",
        "  def __init__(self, size, dropout):\n",
        "    super().__init__()\n",
        "    self.norm = LayerNorm(size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    # sublayer: self-attn or ffn\n",
        "    return x + self.dropout(sublayer(self.norm(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Qh1VW_EZsEP_"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, size, self_attn, feed_forward, dropout=0.0):\n",
        "    super().__init__()\n",
        "    self.self_attn = self_attn\n",
        "    self.feed_forward = feed_forward\n",
        "\n",
        "    self.sublayer = nn.ModuleList([\n",
        "        SublayerConnection(size, dropout),\n",
        "        SublayerConnection(size, dropout)\n",
        "    ])\n",
        "\n",
        "    self.size = size\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    # 1. x' = x + self-attention(LN(x))\n",
        "    x = self.sublayer[0](\n",
        "        x,\n",
        "        lambda x: self.self_attn(x, x, x, mask)[0]\n",
        "    )\n",
        "    # 2. x' + FFN(LN(x))\n",
        "    return self.sublayer[1](x, self.feed_forward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA44YeGXAvfd"
      },
      "source": [
        "## 4.2 Encoder Stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "68HjvirD1dkc"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, layer, N):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([\n",
        "      copy.deepcopy(layer) for _ in range(N)\n",
        "    ])\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJrfvpC6OBfi"
      },
      "source": [
        "# 阶段5: Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joROj-v-OI1n"
      },
      "source": [
        "## 5.1 Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mfAaijdeOMVd"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout=0.0):\n",
        "      super().__init__()\n",
        "      self.self_attn = self_attn\n",
        "      self.src_attn = src_attn\n",
        "      self.feed_forward = feed_forward\n",
        "\n",
        "      self.sublayer = nn.ModuleList([\n",
        "          SublayerConnection(size, dropout), # self-attn\n",
        "          SublayerConnection(size, dropout), # cross-attn\n",
        "          SublayerConnection(size, dropout), # ffn\n",
        "      ])\n",
        "\n",
        "      self.size = size\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "      m = memory\n",
        "      # 1. 第一个子层连接：自注意力机制 x = x + self-attention(LN(x))\n",
        "      x = self.sublayer[0](\n",
        "          x,\n",
        "          lambda x: self.self_attn(x, x, x, tgt_mask)[0]\n",
        "        )\n",
        "      # 2. 第二个子层连接：交叉注意力机制 x = x + cross-attention(LN(x))\n",
        "      x = self.sublayer[1](\n",
        "          x,\n",
        "          lambda x: self.src_attn(x, m, m, src_mask)[0]\n",
        "        )\n",
        "\n",
        "      # 3. 第三个子层连接：前馈神经网络 x + FFN(LN(x))\n",
        "      return self.sublayer[2](x, self.feed_forward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj2CTogDvvSl"
      },
      "source": [
        "## 2.2 Decoder Stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2n2HPfQ4v4Jp"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, layer, N):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([\n",
        "        copy.deepcopy(layer) for _ in range(N)\n",
        "    ])\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self, x, memory, src_mask, tgt_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, memory, src_mask, tgt_mask)\n",
        "    return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwbh2gbq6t0h"
      },
      "source": [
        "# 阶段6: Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HzxkIdfF6y_r"
      },
      "outputs": [],
      "source": [
        "# decoder的hidden state转为概率分布\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, d_model, vocab_size):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #return F.log_softmax(self.proj(x), dim=-1)\n",
        "    return self.proj(x) # logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZmIT4Jj8rdx"
      },
      "source": [
        "# 阶段7: 组装Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FScirota8zB_"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,\n",
        "         src_vocab_size,\n",
        "         tgt_vocab_size,\n",
        "         d_model,\n",
        "         d_ff,\n",
        "         N,\n",
        "         h,\n",
        "         dropout):\n",
        "    super().__init__()\n",
        "    self.src_vocab_size = src_vocab_size\n",
        "    self.tgt_vocab_size = tgt_vocab_size\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.src_embed = nn.Sequential(\n",
        "        Embeddings(src_vocab_size, d_model),\n",
        "        PositionalEncoding(d_model, dropout)\n",
        "    )\n",
        "\n",
        "    self.tgt_embed = nn.Sequential(\n",
        "        Embeddings(tgt_vocab_size, d_model),\n",
        "        PositionalEncoding(d_model, dropout)\n",
        "    )\n",
        "\n",
        "    self.encoder = Encoder(\n",
        "        EncoderLayer(d_model,\n",
        "               MultiHeadAttention(d_model, h),\n",
        "               PositionwiseFeedForward(d_model, d_ff),\n",
        "               dropout),\n",
        "        N\n",
        "    )\n",
        "\n",
        "    self.decoder = Decoder(\n",
        "        DecoderLayer(d_model,\n",
        "               MultiHeadAttention(d_model, h),\n",
        "               MultiHeadAttention(d_model, h),\n",
        "               PositionwiseFeedForward(d_model, d_ff),\n",
        "               dropout),\n",
        "        N\n",
        "    )\n",
        "\n",
        "    self.generator = Generator(d_model, tgt_vocab_size)\n",
        "    \n",
        "    self._init_weights()\n",
        "    self._tie_weights()\n",
        "\n",
        "  def _init_weights(self):\n",
        "    # 初始化模型权重\n",
        "    for p in self.parameters():\n",
        "      if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "    # 特别处理嵌入层\n",
        "    nn.init.normal_(self.src_embed[0].lut.weight, mean=0, std=self.d_model ** -0.5)\n",
        "    nn.init.normal_(self.tgt_embed[0].lut.weight, mean=0, std=self.d_model ** -0.5)\n",
        "\n",
        "\n",
        "  def _tie_weights(self):\n",
        "    self.generator.proj.weight = self.tgt_embed[0].lut.weight\n",
        "    self.generator.proj.weight.requires_grad = True\n",
        "\n",
        "  def forward(self, src, tgt, src_mask, tgt_mask, cross_attn_mask):\n",
        "    enc_output = self.encoder(self.src_embed(src), src_mask)\n",
        "    dec_output = self.decoder(\n",
        "        self.tgt_embed(tgt),\n",
        "        enc_output,\n",
        "        cross_attn_mask,\n",
        "        tgt_mask\n",
        "      )\n",
        "\n",
        "    logits = self.generator(dec_output)\n",
        "    return logits #(batch, seq_len, tgt_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDcsGI2FMYKN"
      },
      "source": [
        "# 阶段8: Label Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class LabelSmoothing(nn.Module):\n",
        "#     def __init__(self, vocab_size, pad_idx, smoothing=0.1):\n",
        "#         super().__init__()\n",
        "#         self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
        "#         self.padding_idx = pad_idx\n",
        "#         self.confidence = 1.0 - smoothing\n",
        "#         self.smoothing = smoothing\n",
        "#         self.size = vocab_size\n",
        "    \n",
        "#     def forward(self, x, target):\n",
        "#         assert x.size(1) == self.size\n",
        "#         true_dist = x.data.clone()\n",
        "#         true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "#         true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "#         true_dist[:, self.padding_idx] = 0\n",
        "#         mask = torch.nonzero(target.data == self.padding_idx)\n",
        "#         if mask.dim() > 0:\n",
        "#             true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "#         return self.criterion(x, true_dist.clone().detach())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7Yekg3JalHt"
      },
      "source": [
        "# 阶段9 : 优化器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Q4aPj9_rardX"
      },
      "outputs": [],
      "source": [
        "# Warmup: 线性增长\n",
        "# lr = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))\n",
        "def make_noam_lambda(d_model, warmup, factor=1.0):\n",
        "  def rate(step):\n",
        "    step = max(step, 1)\n",
        "    return factor * (\n",
        "        d_model ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
        "    )\n",
        "  return rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "fqnAgbgFcDRg",
        "outputId": "6ccd205a-b2e0-48e0-ffa4-8946e8a2fff1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# learning rate warmup可视化\n",
        "model = torch.nn.Linear(10, 10)\n",
        "\n",
        "d_model = 512\n",
        "warmup_steps = 4000\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1.0)\n",
        "scheduler = LambdaLR(\n",
        "    optimizer,\n",
        "    lr_lambda=make_noam_lambda(d_model, warmup_steps)\n",
        ")\n",
        "\n",
        "lrs = []\n",
        "\n",
        "for step in range(1, 20000):\n",
        "    optimizer.step()\n",
        "    scheduler.step()     # 更新 lr\n",
        "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(lrs)\n",
        "plt.axvline(warmup_steps, color='r', linestyle='--', label='warmup end')\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.ylabel(\"Learning rate\")\n",
        "plt.title(\"Noam Learning Rate Schedule\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2oj_YQZNYRD"
      },
      "source": [
        "# 最终阶段： 中英翻译Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 数据处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609,
          "referenced_widgets": [
            "7910ee144af24a0daea7f552a1e1678a",
            "2b9d6ba9959048afa09ee9e3ee725549",
            "ee5fd467a18c4fa2870d3950883e56b8",
            "05991c9d98df40279900904568043a82",
            "e3fb3b7a71a049029f99dfdd547b9567",
            "92c750d6b4da47a1ae4aada8f4a1bde8",
            "369a3407998f4d9ca1418dfda31041cc",
            "f015f6c4f1a34c38aec90330cae84447",
            "81c4e90790364ad68276ad7d5756e350",
            "018c739bc15f48ceabb6baf3c89a0471",
            "a01a7e3e293e44f28e7b0df8e15c667d"
          ]
        },
        "id": "zowlTJftoAui",
        "outputId": "070fcdc2-889c-4a66-c7dd-557412d499d9"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"wmt19\", \"zh-en\", split=\"train\")\n",
        "\n",
        "sampled_for_step = dataset.shuffle(seed=42).select(range(100000))\n",
        "\n",
        "\n",
        "\n",
        "# 提取中英文句子并写入临时文件\n",
        "with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as f:\n",
        "    temp_file = f.name\n",
        "    for item in sampled_for_step:\n",
        "        # 写入中文句子\n",
        "        f.write(item['translation']['zh'].strip() + '\\n')\n",
        "        # 写入英文句子\n",
        "        f.write(item['translation']['en'].strip() + '\\n')\n",
        "\n",
        "print(f\"已写入 {len(sampled_for_step)*2} 行文本（中英交替）到临时文件\")\n",
        "\n",
        "# 训练 SentencePiece 模型\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=temp_file,\n",
        "    model_prefix=\"sp16k\",\n",
        "    vocab_size=16000,\n",
        "    model_type=\"unigram\",\n",
        "    character_coverage=0.9995,\n",
        "    pad_id=0, bos_id=1, eos_id=2, unk_id=3,\n",
        "    shuffle_input_sentence=True,\n",
        "    num_threads=4\n",
        ")\n",
        "\n",
        "# 4. 清理临时文件\n",
        "os.unlink(temp_file)\n",
        "print(\"sp16k.model 构建完成\")\n",
        "\n",
        "# 5. 验证模型\n",
        "sp = spm.SentencePieceProcessor(model_file=\"sp16k.model\")\n",
        "PAD, BOS, EOS, UNK = sp.pad_id(), sp.bos_id(), sp.eos_id(), sp.unk_id()\n",
        "print(\"词表大小:\", sp.get_piece_size())\n",
        "print(\"中文分词:\", sp.encode(\"我爱深度学习\", out_type=str))\n",
        "print(\"英文分词:\", sp.encode(\"I love deep learning\", out_type=str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "中文: ['▁我', '爱', '深', '度', '学习']\n",
            "英文: ['▁I', '▁love', '▁deep', '▁learning']\n"
          ]
        }
      ],
      "source": [
        "# 数据处理\n",
        "def zh_tokenize(sentence):\n",
        "    return sp.encode(sentence.strip(), out_type=str)\n",
        "\n",
        "def en_tokenize(sentence):\n",
        "    return sp.encode(sentence.strip(), out_type=str)\n",
        "\n",
        "print(\"中文:\", zh_tokenize(\"我爱深度学习\"))      # ['▁我', '爱', '深度', '学习']\n",
        "print(\"英文:\", en_tokenize(\"I love deep learning\"))  # ['▁I', '▁love', '▁deep', '▁learning']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, tgt_sentences, max_src_len=256, max_tgt_len=256):\n",
        "        self.data = []\n",
        "        dropped = 0\n",
        "        \n",
        "        for zh, en in zip(src_sentences, tgt_sentences):\n",
        "            # 使用 SP 编码（自动处理 <bos>/<eos>）\n",
        "            src_ids = sp.encode(zh.strip(), out_type=int)  # list of int\n",
        "            tgt_ids = sp.encode(en.strip(), out_type=int)\n",
        "            \n",
        "            src_ids = [BOS] + src_ids + [EOS]\n",
        "            tgt_ids = [BOS] + tgt_ids + [EOS]\n",
        "            \n",
        "            if len(src_ids) > max_src_len or len(tgt_ids) > max_tgt_len:\n",
        "                dropped += 1\n",
        "                continue\n",
        "            if len(tgt_ids) < 3:\n",
        "                dropped += 1\n",
        "                continue\n",
        "\n",
        "            tgt_input = tgt_ids[:-1]\n",
        "            tgt_output = tgt_ids[1:]\n",
        "            \n",
        "            self.data.append((\n",
        "                torch.tensor(src_ids),\n",
        "                torch.tensor(tgt_input),\n",
        "                torch.tensor(tgt_output)\n",
        "            ))\n",
        "        \n",
        "        print(f\"[TranslationDataset] 保留 {len(self.data)} 条样本，丢弃 {dropped} 条超长样本\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "u0YmgpHo5vof"
      },
      "outputs": [],
      "source": [
        "def make_collate_fn(pad_id):\n",
        "    def collate_fn(batch):\n",
        "        srcs, tgts_input, tgts_output = zip(*batch)\n",
        "        srcs_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "            srcs, batch_first=True, padding_value=pad_id\n",
        "        )\n",
        "        tgts_input_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "            tgts_input, batch_first=True, padding_value=pad_id\n",
        "        )\n",
        "        tgts_output_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "            tgts_output, batch_first=True, padding_value=pad_id\n",
        "        )\n",
        "        return srcs_padded, tgts_input_padded, tgts_output_padded\n",
        "    return collate_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVhKcOAC6tOQ"
      },
      "source": [
        "## 训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleLossCompute: \n",
        "    def __init__(self, criterion): \n",
        "        self.criterion = criterion\n",
        "\n",
        "    def __call__(self, x, y, norm):\n",
        "        \"\"\"\n",
        "        x: 模型输出的logits [batch_size, seq_len, tgt_vocab_size]\n",
        "        y: 目标token [batch_size, seq_len]\n",
        "        norm: 归一化因子（ntokens）\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, vocab_size = x.shape\n",
        "        x = x.view(-1, vocab_size)      # -> [N, C]\n",
        "        y = y.view(-1)                  # -> [N]\n",
        "\n",
        "        loss = self.criterion(x, y)  # 这是sum reduction的总损失\n",
        "        \n",
        "        # 计算平均损失（每个token）\n",
        "        loss_per_token = loss / norm if norm > 0 else loss\n",
        "        \n",
        "        # 返回：(用于记录的标量, 用于反向传播的tensor)\n",
        "        return loss_per_token.item(), loss_per_token\n",
        "    \n",
        "@dataclass\n",
        "class TrainState: \n",
        "    # 跟踪训练状态\n",
        "    step: int = 0 \n",
        "    accum_step: int = 0 \n",
        "    samples: int = 0 \n",
        "    tokens: int = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BucketSampler(Sampler):\n",
        "    def __init__(self, dataset, max_tokens, max_batch_size=64, shuffle=True, drop_last=False):\n",
        "        self.dataset = dataset\n",
        "        self.max_tokens = max_tokens\n",
        "        self.max_batch_size = max_batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.drop_last = drop_last\n",
        "        \n",
        "        # 获取所有样本长度（取 source 和 target 的最大值）\n",
        "        self.lengths = []\n",
        "        for i in range(len(dataset)):\n",
        "            src_len = len(dataset[i][0])\n",
        "            tgt_len = len(dataset[i][1])\n",
        "            self.lengths.append(src_len + tgt_len)\n",
        "        \n",
        "        # 按长度排序索引\n",
        "        sorted_indices = sorted(range(len(self.lengths)), key=lambda i: self.lengths[i])\n",
        "        \n",
        "        # 创建批次\n",
        "        self.batches = []\n",
        "        current_batch = []\n",
        "        current_total_tokens = 0\n",
        "        \n",
        "        for idx in sorted_indices:\n",
        "            seq_len = self.lengths[idx]\n",
        "            # 估算添加此样本后的总 token 数\n",
        "            new_total_tokens = current_total_tokens + seq_len\n",
        "            \n",
        "            # 如果超过最大 token 数或 batch size 限制，保存当前 batch\n",
        "            if (new_total_tokens > self.max_tokens or \n",
        "                len(current_batch) >= self.max_batch_size) and current_batch:\n",
        "                self.batches.append(current_batch)\n",
        "                current_batch = [idx]\n",
        "                current_total_tokens = seq_len\n",
        "            else:\n",
        "                current_batch.append(idx)\n",
        "                current_total_tokens = new_total_tokens\n",
        "        \n",
        "        # 添加最后一批\n",
        "        if current_batch and (not drop_last or len(current_batch) >= 2):\n",
        "            self.batches.append(current_batch)\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.batches)\n",
        "        for batch in self.batches:\n",
        "            yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batches)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sacrebleu\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import heapq\n",
        "\n",
        "# 质量监控\n",
        "class ValidationMonitor:\n",
        "    def __init__(self, sp_processor, device, test_set=None, patience=5, min_epochs=3, beam_size=3):\n",
        "        self.sp = sp_processor\n",
        "        self.device = device\n",
        "        self.patience = patience\n",
        "        self.min_epochs = min_epochs\n",
        "        self.beam_size = beam_size\n",
        "        self.best_bleu = 0.0\n",
        "        self.wait = 0\n",
        "        self.history = []\n",
        "        \n",
        "        # 参考测试集\n",
        "        self.test_set = test_set or [\n",
        "            (\"我爱深度学习\", \"I love deep learning\"),\n",
        "            (\"人工智能改变世界\", \"Artificial intelligence is changing the world\"),\n",
        "            (\"今天天气真不错\", \"The weather is really nice today\"),\n",
        "            (\"经济危机不断加深\", \"The economic crisis is deepening\"),\n",
        "            (\"机器学习是人工智能的核心\", \"Machine learning is the core of artificial intelligence\"),\n",
        "            (\"自然语言处理很有趣\", \"Natural language processing is fascinating\"),\n",
        "            (\"深度学习需要大量数据\", \"Deep learning requires massive amounts of data\"),\n",
        "            (\"神经网络模仿人脑工作\", \"Neural networks mimic the human brain\"),\n",
        "            (\"Transformer模型改变了NLP领域\", \"Transformer models have revolutionized the NLP field\"),\n",
        "            (\"注意力机制是关键创新\", \"The attention mechanism is a key innovation\"),\n",
        "            (\"梯度下降优化模型参数\", \"Gradient descent optimizes model parameters\"),\n",
        "            (\"过拟合会导致泛化能力差\", \"Overfitting leads to poor generalization\"),\n",
        "            (\"正则化技术提高模型鲁棒性\", \"Regularization techniques improve model robustness\"),\n",
        "            (\"批量归一化加速训练\", \"Batch normalization accelerates training\"),\n",
        "            (\"残差连接解决梯度消失\", \"Residual connections solve the vanishing gradient problem\"),\n",
        "            (\"词嵌入捕捉语义信息\", \"Word embeddings capture semantic information\"),\n",
        "            (\"序列到序列模型用于机器翻译\", \"Sequence-to-sequence models are used for machine translation\"),\n",
        "            (\"束搜索提高翻译质量\", \"Beam search improves translation quality\"),\n",
        "            (\"自回归生成保证连贯性\", \"Autoregressive generation ensures coherence\"),\n",
        "            (\"多头注意力增强表示能力\", \"Multi-head attention enhances representation capability\")\n",
        "        ]\n",
        "\n",
        "        # 预处理参考译文 (分词处理)\n",
        "        self.ref_sentences = [\n",
        "            self._preprocess_ref(ref) for _, ref in self.test_set\n",
        "        ]\n",
        "\n",
        "    def _preprocess_ref(self, sentence: str) -> str:\n",
        "        # 英文分词\n",
        "        tokens = self.sp.encode(sentence.strip(), out_type=str)\n",
        "        tokens = [t for t in tokens if t not in ['<pad>', '<bos>', '<eos>', '<unk>']]\n",
        "        return \" \".join(tokens).replace(\"▁\", \" \").strip()\n",
        "\n",
        "    def compute_bleu(self, model, src_sentences: List[str], tgt_references: List[str]) -> float:\n",
        "        model.eval()\n",
        "        translations = []\n",
        "        with torch.no_grad():\n",
        "            for sent in src_sentences:\n",
        "                # 使用beam search (beam_size=3)\n",
        "                token_ids = self.beam_decode(model, sent, beam_size=self.beam_size)\n",
        "                # 转换为文本\n",
        "                trans_text = self.sp.decode(token_ids)\n",
        "                translations.append(trans_text)\n",
        "\n",
        "        processed_trans = [\n",
        "            self._preprocess_ref(trans) for trans in translations\n",
        "        ]\n",
        "\n",
        "        # 计算BLEU\n",
        "        bleu = sacrebleu.corpus_bleu(\n",
        "            processed_trans,\n",
        "            [self.ref_sentences],\n",
        "            tokenize='none',\n",
        "            lowercase=True\n",
        "        )\n",
        "        return bleu.score\n",
        "    \n",
        "\n",
        "    def beam_decode(self, model, sentence: str, beam_size: int = 3, max_len: int = 50) -> List[int]:\n",
        "        model.eval()\n",
        "        \n",
        "        # 1. 编码源句子\n",
        "        src_ids = self.sp.encode(sentence.strip(), out_type=int)\n",
        "        src_ids = [self.sp.bos_id()] + src_ids + [self.sp.eos_id()]\n",
        "        src_tensor = torch.tensor([src_ids], dtype=torch.long, device=self.device)\n",
        "        \n",
        "        # 2. 创建mask\n",
        "        src_mask = make_src_mask(src_tensor, self.sp.pad_id()).to(self.device)\n",
        "        \n",
        "        # 3. 初始化beam\n",
        "        BOS = self.sp.bos_id()\n",
        "        EOS = self.sp.eos_id()\n",
        "        PAD = self.sp.pad_id()\n",
        "        UNK = self.sp.unk_id()\n",
        "        \n",
        "        # 每个候选: (log_prob, tokens, finished)\n",
        "        beams = [(0.0, [BOS], False)]\n",
        "        finished_beams = []\n",
        "        \n",
        "        # 4. 迭代生成\n",
        "        for step in range(max_len):\n",
        "            if len(finished_beams) >= beam_size:\n",
        "                break\n",
        "                \n",
        "            # 收集所有未完成候选\n",
        "            current_inputs = []\n",
        "            current_beams = []\n",
        "            \n",
        "            for log_prob, tokens, finished in beams:\n",
        "                if not finished:\n",
        "                    current_inputs.append(tokens)\n",
        "                    current_beams.append((log_prob, tokens))\n",
        "            \n",
        "            if not current_inputs:\n",
        "                break\n",
        "                \n",
        "            # 准备batch输入\n",
        "            max_len_curr = max(len(tokens) for tokens in current_inputs)\n",
        "            batch_input = torch.full(\n",
        "                (len(current_inputs), max_len_curr), \n",
        "                PAD, \n",
        "                dtype=torch.long, \n",
        "                device=self.device\n",
        "            )\n",
        "            \n",
        "            for i, tokens in enumerate(current_inputs):\n",
        "                batch_input[i, :len(tokens)] = torch.tensor(tokens, device=self.device)\n",
        "            \n",
        "            # 创建target mask\n",
        "            tgt_mask = make_tgt_mask(batch_input, PAD).to(self.device)\n",
        "            cross_mask = make_cross_attn_mask(src_tensor.repeat(len(current_inputs), 1), batch_input, PAD).to(self.device)\n",
        "            \n",
        "            # 模型前向\n",
        "            with torch.no_grad():\n",
        "                logits = model(\n",
        "                    src_tensor.repeat(len(current_inputs), 1),\n",
        "                    batch_input,\n",
        "                    src_mask.repeat(len(current_inputs), 1, 1, 1),\n",
        "                    tgt_mask,\n",
        "                    cross_mask\n",
        "                )\n",
        "            \n",
        "            # 获取最后一步的logits\n",
        "            next_logits = logits[:, -1, :]  # [batch_size, vocab_size]\n",
        "            \n",
        "            # 应用约束\n",
        "            next_logits[:, PAD] = -1e9\n",
        "            next_logits[:, BOS] = -1e9\n",
        "            next_logits[:, UNK] = -1e9\n",
        "            \n",
        "            # 长度惩罚因子 (避免过短序列)\n",
        "            alpha = 0.7\n",
        "            \n",
        "            # 扩展候选\n",
        "            new_beams = []\n",
        "            for i, (log_prob, tokens) in enumerate(current_beams):\n",
        "                # 获取top-k候选 (k = beam_size * 2)\n",
        "                probs = F.log_softmax(next_logits[i], dim=-1)\n",
        "                topk_probs, topk_ids = torch.topk(probs, k=beam_size * 2, dim=-1)\n",
        "                \n",
        "                for prob, token_id in zip(topk_probs, topk_ids):\n",
        "                    new_log_prob = log_prob + prob.item()\n",
        "                    # 长度归一化: (5 + len)^alpha / (5 + 1)^alpha\n",
        "                    normalized_prob = new_log_prob / ((5 + len(tokens)) ** alpha)\n",
        "                    \n",
        "                    new_tokens = tokens + [token_id.item()]\n",
        "                    finished = (token_id.item() == EOS) or (len(new_tokens) >= max_len)\n",
        "                    \n",
        "                    if finished:\n",
        "                        finished_beams.append((normalized_prob, new_tokens))\n",
        "                    else:\n",
        "                        new_beams.append((new_log_prob, new_tokens, False))\n",
        "            \n",
        "            # 5. 合并并选择top-k\n",
        "            beams = heapq.nlargest(beam_size, new_beams, key=lambda x: x[0])\n",
        "            \n",
        "            # 6. 检查早停\n",
        "            if all(finished for _, _, finished in beams):\n",
        "                break\n",
        "        \n",
        "        # 7. 选择最佳完成序列\n",
        "        if finished_beams:\n",
        "            best_beam = max(finished_beams, key=lambda x: x[0])[1]\n",
        "        else:\n",
        "            best_beam = max(beams, key=lambda x: x[0])[1]\n",
        "        \n",
        "        # 移除BOS/EOS/PAD\n",
        "        return [\n",
        "            token_id for token_id in best_beam \n",
        "            if token_id not in [BOS, EOS, PAD]\n",
        "        ]\n",
        "    \n",
        "def should_stop(self, epoch, val_loss, model):\n",
        "    src_sents, _ = zip(*self.test_set)\n",
        "    current_bleu = self.compute_bleu(model, list(src_sents), self.ref_sentences)\n",
        "    \n",
        "    # 保存历史\n",
        "    self.history.append({\n",
        "        'epoch': epoch,\n",
        "        'val_loss': val_loss,\n",
        "        'bleu': current_bleu\n",
        "    })\n",
        "    \n",
        "    # 打印验证结果\n",
        "    print(f\"\\n[VAL] Epoch {epoch+1} | Val Loss: {val_loss:.4f} | BLEU-4: {current_bleu:.2f}\")\n",
        "    print(f\"Best BLEU so far: {self.best_bleu:.2f}\")\n",
        "    \n",
        "    # 保存最佳模型条件\n",
        "    save_model = False\n",
        "    if current_bleu > self.best_bleu:\n",
        "        self.best_bleu = current_bleu\n",
        "        self.wait = 0\n",
        "        save_model = True\n",
        "    else:\n",
        "        self.wait += 1\n",
        "    \n",
        "    # 早停条件\n",
        "    stop_training = False\n",
        "    if epoch >= self.min_epochs and self.wait >= self.patience:\n",
        "        stop_training = True\n",
        "    \n",
        "    return stop_training, save_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_epoch(\n",
        "    data_iter,\n",
        "    model,\n",
        "    loss_compute,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    mode=\"train\",\n",
        "    accum_iter=1,\n",
        "    train_state=None,\n",
        "    PAD=0,\n",
        "    device=device,\n",
        "    make_src_mask_fn=make_src_mask,\n",
        "    make_tgt_mask_fn=make_tgt_mask,\n",
        "    make_cross_attn_mask_fn=make_cross_attn_mask\n",
        "):\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    n_accum = 0\n",
        "\n",
        "    # 训练模式清零梯度\n",
        "    if mode == \"train\" and optimizer is not None:\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for i, batch_data in enumerate(data_iter):\n",
        "        src = batch_data[0].to(device, non_blocking=True)\n",
        "        tgt_input = batch_data[1].to(device, non_blocking=True)\n",
        "        tgt_y = batch_data[2].to(device, non_blocking=True)\n",
        "\n",
        "        batch = Batch(src, tgt_input, tgt_y, PAD, make_src_mask_fn, make_tgt_mask_fn, make_cross_attn_mask_fn)\n",
        "        \n",
        "        # 前向传播 (纯FP32)\n",
        "        out = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask, batch.cross_attn_mask)\n",
        "        loss_value, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
        "\n",
        "        if mode == \"train\" or mode == \"train+log\":\n",
        "            # 反向传播\n",
        "            loss_node.backward()\n",
        "            \n",
        "            train_state.step += 1\n",
        "            train_state.samples += batch.src.shape[0]\n",
        "            train_state.tokens += batch.ntokens\n",
        "\n",
        "            if (i + 1) % accum_iter == 0:\n",
        "                # 梯度裁剪 (防止梯度爆炸)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                \n",
        "                # 优化器步进\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                n_accum += 1\n",
        "                train_state.accum_step += 1\n",
        "            \n",
        "                # 学习率调度\n",
        "                if scheduler is not None:\n",
        "                    scheduler.step()\n",
        "\n",
        "        total_loss += loss_value * batch.ntokens\n",
        "        total_tokens += batch.ntokens\n",
        "        tokens += batch.ntokens\n",
        "\n",
        "        if i % 200 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
        "            lr = optimizer.param_groups[0][\"lr\"] if optimizer else 0.0\n",
        "            elapsed = time.time() - start\n",
        "            print(\n",
        "                f\"Epoch Step: {i} | Accumulation Step: {n_accum} | \"\n",
        "                f\"Loss: {loss_value:.4f} | \"\n",
        "                f\"Tokens/Sec: {tokens / elapsed:.0f} | \"\n",
        "                f\"Learning Rate: {lr:.8f}\"\n",
        "            )\n",
        "            start = time.time()\n",
        "            tokens = 0\n",
        "\n",
        "    return total_loss / total_tokens, train_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate(model, sentence, sp_processor, device, beam_size=3, max_len=50):\n",
        "    #  使用beam search\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # 1. 编码源句子\n",
        "        src_ids = sp_processor.encode(sentence.strip(), out_type=int)\n",
        "        src_ids = [sp_processor.bos_id()] + src_ids + [sp_processor.eos_id()]\n",
        "        \n",
        "        # 2. 使用Beam Search (复用ValidationMonitor)\n",
        "        monitor = ValidationMonitor(sp_processor, device, beam_size=beam_size)\n",
        "        token_ids = monitor.beam_decode(model, sentence, beam_size=beam_size, max_len=max_len)\n",
        "        \n",
        "        # 3. 转换为文本\n",
        "        return sp_processor.decode(token_ids)\n",
        "\n",
        "def evaluate(model, sp_processor, device, epoch, val_monitor):\n",
        "    # 使用BLEU评估翻译质量\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Epoch {epoch+1} 翻译质量评估\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # 计算当前BLEU\n",
        "        src_sents, _ = zip(*val_monitor.test_set)\n",
        "        bleu_score = val_monitor.compute_bleu(model, list(src_sents), val_monitor.ref_sentences)\n",
        "        \n",
        "        # 打印关键样本\n",
        "        print(f\"BLEU-4 Score: {bleu_score:.2f}\")\n",
        "        print(\"\\n关键样本翻译:\")\n",
        "        for sent in [\"我爱深度学习\", \"人工智能改变世界\", \"经济危机不断加深\"]:\n",
        "            translation = translate(model, sent, sp_processor, device, beam_size=3)\n",
        "            print(f\"【{sent}】 → {translation}\")\n",
        "    \n",
        "    model.train()\n",
        "    return bleu_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    criterion,\n",
        "    num_epochs=30,\n",
        "    accum_iter=2,\n",
        "    device=device,\n",
        "    PAD=0,\n",
        "    val_monitor=None\n",
        "):\n",
        "    # 创建loss compute\n",
        "    loss_compute = SimpleLossCompute(criterion)\n",
        "    \n",
        "    # 训练状态\n",
        "    train_state = TrainState()\n",
        "    \n",
        "    # 验证监控\n",
        "    if val_monitor is None:\n",
        "        val_monitor = ValidationMonitor(sp, device, beam_size=3, patience=5, min_epochs=8)\n",
        "    \n",
        "    best_bleu = 0.0\n",
        "        \n",
        "    # 重置 scheduler 状态\n",
        "    if scheduler is not None:\n",
        "        scheduler.last_epoch = -1  # 重置 step 计数器\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        if epoch == 0:\n",
        "            print(\"Initial parameter ranges:\")\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.numel() > 0:\n",
        "                    print(f\"{name}: min={param.min().item():.4f}, max={param.max().item():.4f}\")\n",
        "                    \n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        # 训练\n",
        "        model.train()\n",
        "        train_loss, train_state = run_epoch(\n",
        "            train_loader,\n",
        "            model,\n",
        "            loss_compute,\n",
        "            optimizer,\n",
        "            scheduler,\n",
        "            mode=\"train+log\",\n",
        "            accum_iter=accum_iter,\n",
        "            train_state=train_state,\n",
        "            PAD=PAD,\n",
        "            device=device,\n",
        "            make_src_mask_fn=make_src_mask,\n",
        "            make_tgt_mask_fn=make_tgt_mask,\n",
        "            make_cross_attn_mask_fn=make_cross_attn_mask\n",
        "        )\n",
        "        \n",
        "        # 验证\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss, _ = run_epoch(\n",
        "                val_loader,\n",
        "                model,\n",
        "                loss_compute,\n",
        "                None,\n",
        "                None,\n",
        "                mode=\"eval\",\n",
        "                accum_iter=1,\n",
        "                PAD=PAD,\n",
        "                device=device,\n",
        "                make_src_mask_fn=make_src_mask,\n",
        "                make_tgt_mask_fn=make_tgt_mask,\n",
        "                make_cross_attn_mask_fn=make_cross_attn_mask\n",
        "            )\n",
        "        \n",
        "        print(f\"\\nEpoch {epoch+1} 完成:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "        \n",
        "        # BLEU评估和早停\n",
        "        stop_training, save_model = val_monitor.should_stop(epoch, val_loss, model)\n",
        "        \n",
        "        # 保存最佳模型\n",
        "        if save_model:\n",
        "            best_bleu = val_monitor.best_bleu\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'bleu': best_bleu,\n",
        "                'train_state': train_state,\n",
        "                'config': config\n",
        "            }, 'best_model.pt')\n",
        "            print(f\"保存新最佳模型 (BLEU: {best_bleu:.2f})\")\n",
        "        \n",
        "        if stop_training:\n",
        "            print(f\"\\n早停触发! 最佳BLEU: {best_bleu:.2f} @ epoch {epoch+1}\")\n",
        "            break\n",
        "        \n",
        "        # 定期展示翻译示例\n",
        "        if epoch % 2 == 0:\n",
        "            evaluate(model, sp, device, epoch, val_monitor)\n",
        "    \n",
        "    return train_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "原始数据: 100000 条\n",
            "词表大小: 16000\n",
            "[TranslationDataset] 保留 95000 条样本，丢弃 0 条超长样本\n",
            "[TranslationDataset] 保留 5000 条样本，丢弃 0 条超长样本\n",
            "训练集大小: 95000\n",
            "验证集大小: 5000\n",
            "训练批次数: 3025 (动态batch)\n",
            "验证批次数: 161\n",
            "训练集样本分布: 最短=3 tokens, 最长=193 tokens\n",
            "示例batch形状: src=torch.Size([37, 30]), tgt_input=torch.Size([37, 35]), tgt_output=torch.Size([37, 35])\n",
            "实际batch tokens数: 1110\n"
          ]
        }
      ],
      "source": [
        "# 划分数据\n",
        "subset_size = min(100000, len(dataset))\n",
        "zh_sentences = [item['translation']['zh'] for item in dataset.select(range(subset_size))]\n",
        "en_sentences = [item['translation']['en'] for item in dataset.select(range(subset_size))]\n",
        "print(f\"原始数据: {len(zh_sentences)} 条\")\n",
        "\n",
        "train_size = int(0.95 * len(zh_sentences))\n",
        "zh_train, en_train = zh_sentences[:train_size], en_sentences[:train_size]\n",
        "zh_val, en_val = zh_sentences[train_size:], en_sentences[train_size:]\n",
        "\n",
        "src_vocab_size = tgt_vocab_size = sp.get_piece_size()\n",
        "print(f\"词表大小: {src_vocab_size}\")  # 应为 16000\n",
        "\n",
        "src_id2tok = {i: sp.id_to_piece(i) for i in range(src_vocab_size)}\n",
        "tgt_id2tok = {i: sp.id_to_piece(i) for i in range(tgt_vocab_size)}\n",
        "\n",
        "# 创建数据集\n",
        "train_dataset = TranslationDataset(zh_train, en_train)\n",
        "val_dataset = TranslationDataset(zh_val, en_val)\n",
        "print(f\"训练集大小: {len(train_dataset)}\")\n",
        "print(f\"验证集大小: {len(val_dataset)}\")\n",
        "\n",
        "TRAIN_TOKENS_PER_BATCH = 2048    # 训练时每batch目标token数\n",
        "VAL_TOKENS_PER_BATCH = 4096    # 验证时更大batch (无梯度计算)\n",
        "\n",
        "# 创建collate函数\n",
        "collate_fn = make_collate_fn(PAD)\n",
        "\n",
        "# 创建BucketSampler\n",
        "train_sampler = BucketSampler(\n",
        "    train_dataset, \n",
        "    max_tokens=2048, \n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_sampler = BucketSampler(\n",
        "    val_dataset, \n",
        "    max_tokens=2048,\n",
        "    shuffle=False,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "# 创建DataLoader\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_sampler=train_sampler,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_sampler=val_sampler,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "# 创建验证监控器\n",
        "val_monitor = ValidationMonitor(sp, device, beam_size=3, patience=5, min_epochs=8)\n",
        "\n",
        "print(f\"训练批次数: {len(train_loader)} (动态batch)\")\n",
        "print(f\"验证批次数: {len(val_loader)}\")\n",
        "print(f\"训练集样本分布: 最短={min(len(train_dataset[i][0]) for i in range(len(train_dataset)))} tokens, \"\n",
        "      f\"最长={max(len(train_dataset[i][0]) for i in range(len(train_dataset)))} tokens\")\n",
        "\n",
        "# 检查一个batch示例\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"示例batch形状: src={sample_batch[0].shape}, tgt_input={sample_batch[1].shape}, tgt_output={sample_batch[2].shape}\")\n",
        "print(f\"实际batch tokens数: {sample_batch[0].shape[0] * sample_batch[0].shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d_model: 256\n",
            "d_ff: 1024\n",
            "num_heads: 4\n",
            "num_layers: 4\n",
            "dropout: 0.1\n",
            "warmup_steps: 4000\n"
          ]
        }
      ],
      "source": [
        "# 超参数\n",
        "config = {\n",
        "    'd_model': 256,\n",
        "    'd_ff': 1024,\n",
        "    'h': 4,\n",
        "    'N': 4,\n",
        "    'dropout': 0.1,\n",
        "    'lr': 1.0,\n",
        "    'weight_decay':1e-4,\n",
        "    'warmup_steps': 4000,\n",
        "    'accumulation_steps': 4,\n",
        "    'factor': 1.0,\n",
        "}\n",
        "\n",
        "model = Transformer(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    d_model=config['d_model'],\n",
        "    d_ff=config['d_ff'],\n",
        "    N=config['N'],\n",
        "    h=config['h'],\n",
        "    dropout=config['dropout']\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=(0.9, 0.98),\n",
        "    eps=1e-9,\n",
        "    weight_decay=config['weight_decay']\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer,\n",
        "    lr_lambda=make_noam_lambda(\n",
        "        d_model=config['d_model'],\n",
        "        warmup=config['warmup_steps'],\n",
        "        factor=config['factor']\n",
        "    )\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    ignore_index=PAD, \n",
        "    label_smoothing=0.1,\n",
        "    reduction=\"sum\"\n",
        ")\n",
        "\n",
        "print(f\"d_model: {config['d_model']}\")\n",
        "print(f\"d_ff: {config['d_ff']}\")\n",
        "print(f\"num_heads: {config['h']}\")\n",
        "print(f\"num_layers: {config['N']}\")\n",
        "print(f\"dropout: {config['dropout']}\")\n",
        "print(f\"warmup_steps: {config['warmup_steps']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial parameter ranges:\n",
            "src_embed.0.lut.weight: min=-0.3115, max=0.3094\n",
            "tgt_embed.0.lut.weight: min=-0.3079, max=0.3261\n",
            "encoder.layers.0.self_attn.W_q.weight: min=-0.1083, max=0.1082\n",
            "encoder.layers.0.self_attn.W_q.bias: min=-0.0622, max=0.0620\n",
            "encoder.layers.0.self_attn.W_k.weight: min=-0.1083, max=0.1082\n",
            "encoder.layers.0.self_attn.W_k.bias: min=-0.0624, max=0.0620\n",
            "encoder.layers.0.self_attn.W_v.weight: min=-0.1082, max=0.1082\n",
            "encoder.layers.0.self_attn.W_v.bias: min=-0.0624, max=0.0617\n",
            "encoder.layers.0.self_attn.W_o.weight: min=-0.1083, max=0.1082\n",
            "encoder.layers.0.self_attn.W_o.bias: min=-0.0625, max=0.0611\n",
            "encoder.layers.0.feed_forward.linear1.weight: min=-0.0685, max=0.0685\n",
            "encoder.layers.0.feed_forward.linear1.bias: min=-0.0624, max=0.0625\n",
            "encoder.layers.0.feed_forward.linear2.weight: min=-0.0685, max=0.0685\n",
            "encoder.layers.0.feed_forward.linear2.bias: min=-0.0312, max=0.0308\n",
            "encoder.layers.0.sublayer.0.norm.a_2: min=1.0000, max=1.0000\n",
            "encoder.layers.0.sublayer.0.norm.b_2: min=0.0000, max=0.0000\n",
            "encoder.layers.0.sublayer.1.norm.a_2: min=1.0000, max=1.0000\n",
            "encoder.layers.0.sublayer.1.norm.b_2: min=0.0000, max=0.0000\n",
            "encoder.layers.1.self_attn.W_q.weight: min=-0.1083, max=0.1083\n",
            "encoder.layers.1.self_attn.W_q.bias: min=-0.0622, max=0.0620\n",
            "encoder.layers.1.self_attn.W_k.weight: min=-0.1082, max=0.1083\n",
            "encoder.layers.1.self_attn.W_k.bias: min=-0.0624, max=0.0620\n",
            "encoder.layers.1.self_attn.W_v.weight: min=-0.1083, max=0.1082\n",
            "encoder.layers.1.self_attn.W_v.bias: min=-0.0624, max=0.0617\n",
            "encoder.layers.1.self_attn.W_o.weight: min=-0.1083, max=0.1083\n",
            "encoder.layers.1.self_attn.W_o.bias: min=-0.0625, max=0.0611\n",
            "encoder.layers.1.feed_forward.linear1.weight: min=-0.0685, max=0.0685\n",
            "encoder.layers.1.feed_forward.linear1.bias: min=-0.0624, max=0.0625\n",
            "encoder.layers.1.feed_forward.linear2.weight: min=-0.0685, max=0.0685\n",
            "encoder.layers.1.feed_forward.linear2.bias: min=-0.0312, max=0.0308\n",
            "encoder.layers.1.sublayer.0.norm.a_2: min=1.0000, max=1.0000\n",
            "encoder.layers.1.sublayer.0.norm.b_2: min=0.0000, max=0.0000\n",
            "encoder.layers.1.sublayer.1.norm.a_2: min=1.0000, max=1.0000\n",
            "encoder.layers.1.sublayer.1.norm.b_2: min=0.0000, max=0.0000\n",
            "encoder.layers.2.self_attn.W_q.weight: min=-0.1082, max=0.1082\n",
            "encoder.layers.2.self_attn.W_q.bias: min=-0.0622, max=0.0620\n",
            "encoder.layers.2.self_attn.W_k.weight: min=-0.1082, max=0.1082\n",
            "encoder.layers.2.self_attn.W_k.bias: min=-0.0624, max=0.0620\n",
            "encoder.layers.2.self_attn.W_v.weight: min=-0.1083, max=0.1083\n",
            "encoder.layers.2.self_attn.W_v.bias: min=-0.0624, max=0.0617\n",
            "encoder.layers.2.self_attn.W_o.weight: min=-0.1083, max=0.1082\n",
            "encoder.layers.2.self_attn.W_o.bias: min=-0.0625, max=0.0611\n",
            "encoder.layers.2.feed_forward.linear1.weight: min=-0.0685, max=0.0685\n",
            "encoder.layers.2.feed_forward.linear1.bias: min=-0.0624, max=0.0625\n",
            "encoder.layers.2.feed_forward.linear2.weight: min=-0.0685, max=0.0685\n",
            "encoder.layers.2.feed_forward.linear2.bias: min=-0.0312, max=0.0308\n",
            "encoder.layers.2.sublayer.0.norm.a_2: min=1.0000, max=1.0000\n",
            "encoder.layers.2.sublayer.0.norm.b_2: min=0.0000, max=0.0000\n",
            "encoder.layers.2.sublayer.1.norm.a_2: min=1.0000, max=1.0000\n",
            "encoder.layers.2.sublayer.1.norm.b_2: min=0.0000, max=0.0000\n",
            "encoder.layers.3.self_attn.W_q.weight: min=-0.1082, max=0.1083\n",
            "encoder.layers.3.self_attn.W_q.bias: min=-0.0622, max=0.0620\n",
            "encoder.layers.3.self_attn.W_k.weight: min=-0.1082, max=0.1083\n",
            "encoder.layers.3.self_attn.W_k.bias: min=-0.0624, max=0.0620\n",
            "encoder.layers.3.self_attn.W_v.weight: min=-0.1083, max=0.1083\n",
            "encoder.layers.3.self_attn.W_v.bias: min=-0.0624, max=0.0617\n",
            "encoder.layers.3.self_attn.W_o.weight: min=-0.1083, max=0.1082\n",
            "encoder.layers.3.self_attn.W_o.bias: min=-0.0625, max=0.0611\n",
            "encoder.layers.3.feed_forward.linear1.weight: min=-0.0685, max=0.0685\n",
            "encoder.layers.3.feed_forward.linear1.bias: min=-0.0624, max=0.0625\n",
            "encoder.layers.3.feed_forward.linear2.weight: min=-0.0685, max=0.0685\n",
            "encoder.layers.3.feed_forward.linear2.bias: min=-0.0312, max=0.0308\n",
            "encoder.layers.3.sublayer.0.norm.a_2: min=1.0000, max=1.0000\n",
            "encoder.layers.3.sublayer.0.norm.b_2: min=0.0000, max=0.0000\n",
            "encoder.layers.3.sublayer.1.norm.a_2: min=1.0000, max=1.0000\n",
            "encoder.layers.3.sublayer.1.norm.b_2: min=0.0000, max=0.0000\n",
            "encoder.norm.a_2: min=1.0000, max=1.0000\n",
            "encoder.norm.b_2: min=0.0000, max=0.0000\n",
            "decoder.layers.0.self_attn.W_q.weight: min=-0.1082, max=0.1083\n",
            "decoder.layers.0.self_attn.W_q.bias: min=-0.0622, max=0.0624\n",
            "decoder.layers.0.self_attn.W_k.weight: min=-0.1082, max=0.1082\n",
            "decoder.layers.0.self_attn.W_k.bias: min=-0.0622, max=0.0624\n",
            "decoder.layers.0.self_attn.W_v.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.0.self_attn.W_v.bias: min=-0.0620, max=0.0619\n",
            "decoder.layers.0.self_attn.W_o.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.0.self_attn.W_o.bias: min=-0.0600, max=0.0625\n",
            "decoder.layers.0.src_attn.W_q.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.0.src_attn.W_q.bias: min=-0.0619, max=0.0618\n",
            "decoder.layers.0.src_attn.W_k.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.0.src_attn.W_k.bias: min=-0.0623, max=0.0586\n",
            "decoder.layers.0.src_attn.W_v.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.0.src_attn.W_v.bias: min=-0.0619, max=0.0618\n",
            "decoder.layers.0.src_attn.W_o.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.0.src_attn.W_o.bias: min=-0.0620, max=0.0623\n",
            "decoder.layers.0.feed_forward.linear1.weight: min=-0.0685, max=0.0685\n",
            "decoder.layers.0.feed_forward.linear1.bias: min=-0.0625, max=0.0624\n",
            "decoder.layers.0.feed_forward.linear2.weight: min=-0.0685, max=0.0685\n",
            "decoder.layers.0.feed_forward.linear2.bias: min=-0.0303, max=0.0312\n",
            "decoder.layers.0.sublayer.0.norm.a_2: min=1.0000, max=1.0000\n",
            "decoder.layers.0.sublayer.0.norm.b_2: min=0.0000, max=0.0000\n",
            "decoder.layers.0.sublayer.1.norm.a_2: min=1.0000, max=1.0000\n",
            "decoder.layers.0.sublayer.1.norm.b_2: min=0.0000, max=0.0000\n",
            "decoder.layers.0.sublayer.2.norm.a_2: min=1.0000, max=1.0000\n",
            "decoder.layers.0.sublayer.2.norm.b_2: min=0.0000, max=0.0000\n",
            "decoder.layers.1.self_attn.W_q.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.1.self_attn.W_q.bias: min=-0.0622, max=0.0624\n",
            "decoder.layers.1.self_attn.W_k.weight: min=-0.1083, max=0.1082\n",
            "decoder.layers.1.self_attn.W_k.bias: min=-0.0622, max=0.0624\n",
            "decoder.layers.1.self_attn.W_v.weight: min=-0.1083, max=0.1082\n",
            "decoder.layers.1.self_attn.W_v.bias: min=-0.0620, max=0.0619\n",
            "decoder.layers.1.self_attn.W_o.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.1.self_attn.W_o.bias: min=-0.0600, max=0.0625\n",
            "decoder.layers.1.src_attn.W_q.weight: min=-0.1082, max=0.1083\n",
            "decoder.layers.1.src_attn.W_q.bias: min=-0.0619, max=0.0618\n",
            "decoder.layers.1.src_attn.W_k.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.1.src_attn.W_k.bias: min=-0.0623, max=0.0586\n",
            "decoder.layers.1.src_attn.W_v.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.1.src_attn.W_v.bias: min=-0.0619, max=0.0618\n",
            "decoder.layers.1.src_attn.W_o.weight: min=-0.1083, max=0.1082\n",
            "decoder.layers.1.src_attn.W_o.bias: min=-0.0620, max=0.0623\n",
            "decoder.layers.1.feed_forward.linear1.weight: min=-0.0685, max=0.0685\n",
            "decoder.layers.1.feed_forward.linear1.bias: min=-0.0625, max=0.0624\n",
            "decoder.layers.1.feed_forward.linear2.weight: min=-0.0685, max=0.0685\n",
            "decoder.layers.1.feed_forward.linear2.bias: min=-0.0303, max=0.0312\n",
            "decoder.layers.1.sublayer.0.norm.a_2: min=1.0000, max=1.0000\n",
            "decoder.layers.1.sublayer.0.norm.b_2: min=0.0000, max=0.0000\n",
            "decoder.layers.1.sublayer.1.norm.a_2: min=1.0000, max=1.0000\n",
            "decoder.layers.1.sublayer.1.norm.b_2: min=0.0000, max=0.0000\n",
            "decoder.layers.1.sublayer.2.norm.a_2: min=1.0000, max=1.0000\n",
            "decoder.layers.1.sublayer.2.norm.b_2: min=0.0000, max=0.0000\n",
            "decoder.layers.2.self_attn.W_q.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.2.self_attn.W_q.bias: min=-0.0622, max=0.0624\n",
            "decoder.layers.2.self_attn.W_k.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.2.self_attn.W_k.bias: min=-0.0622, max=0.0624\n",
            "decoder.layers.2.self_attn.W_v.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.2.self_attn.W_v.bias: min=-0.0620, max=0.0619\n",
            "decoder.layers.2.self_attn.W_o.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.2.self_attn.W_o.bias: min=-0.0600, max=0.0625\n",
            "decoder.layers.2.src_attn.W_q.weight: min=-0.1082, max=0.1083\n",
            "decoder.layers.2.src_attn.W_q.bias: min=-0.0619, max=0.0618\n",
            "decoder.layers.2.src_attn.W_k.weight: min=-0.1083, max=0.1082\n",
            "decoder.layers.2.src_attn.W_k.bias: min=-0.0623, max=0.0586\n",
            "decoder.layers.2.src_attn.W_v.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.2.src_attn.W_v.bias: min=-0.0619, max=0.0618\n",
            "decoder.layers.2.src_attn.W_o.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.2.src_attn.W_o.bias: min=-0.0620, max=0.0623\n",
            "decoder.layers.2.feed_forward.linear1.weight: min=-0.0685, max=0.0685\n",
            "decoder.layers.2.feed_forward.linear1.bias: min=-0.0625, max=0.0624\n",
            "decoder.layers.2.feed_forward.linear2.weight: min=-0.0685, max=0.0685\n",
            "decoder.layers.2.feed_forward.linear2.bias: min=-0.0303, max=0.0312\n",
            "decoder.layers.2.sublayer.0.norm.a_2: min=1.0000, max=1.0000\n",
            "decoder.layers.2.sublayer.0.norm.b_2: min=0.0000, max=0.0000\n",
            "decoder.layers.2.sublayer.1.norm.a_2: min=1.0000, max=1.0000\n",
            "decoder.layers.2.sublayer.1.norm.b_2: min=0.0000, max=0.0000\n",
            "decoder.layers.2.sublayer.2.norm.a_2: min=1.0000, max=1.0000\n",
            "decoder.layers.2.sublayer.2.norm.b_2: min=0.0000, max=0.0000\n",
            "decoder.layers.3.self_attn.W_q.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.3.self_attn.W_q.bias: min=-0.0622, max=0.0624\n",
            "decoder.layers.3.self_attn.W_k.weight: min=-0.1083, max=0.1082\n",
            "decoder.layers.3.self_attn.W_k.bias: min=-0.0622, max=0.0624\n",
            "decoder.layers.3.self_attn.W_v.weight: min=-0.1083, max=0.1082\n",
            "decoder.layers.3.self_attn.W_v.bias: min=-0.0620, max=0.0619\n",
            "decoder.layers.3.self_attn.W_o.weight: min=-0.1082, max=0.1082\n",
            "decoder.layers.3.self_attn.W_o.bias: min=-0.0600, max=0.0625\n",
            "decoder.layers.3.src_attn.W_q.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.3.src_attn.W_q.bias: min=-0.0619, max=0.0618\n",
            "decoder.layers.3.src_attn.W_k.weight: min=-0.1082, max=0.1082\n",
            "decoder.layers.3.src_attn.W_k.bias: min=-0.0623, max=0.0586\n",
            "decoder.layers.3.src_attn.W_v.weight: min=-0.1083, max=0.1083\n",
            "decoder.layers.3.src_attn.W_v.bias: min=-0.0619, max=0.0618\n",
            "decoder.layers.3.src_attn.W_o.weight: min=-0.1082, max=0.1083\n",
            "decoder.layers.3.src_attn.W_o.bias: min=-0.0620, max=0.0623\n",
            "decoder.layers.3.feed_forward.linear1.weight: min=-0.0685, max=0.0685\n",
            "decoder.layers.3.feed_forward.linear1.bias: min=-0.0625, max=0.0624\n",
            "decoder.layers.3.feed_forward.linear2.weight: min=-0.0685, max=0.0685\n",
            "decoder.layers.3.feed_forward.linear2.bias: min=-0.0303, max=0.0312\n",
            "decoder.layers.3.sublayer.0.norm.a_2: min=1.0000, max=1.0000\n",
            "decoder.layers.3.sublayer.0.norm.b_2: min=0.0000, max=0.0000\n",
            "decoder.layers.3.sublayer.1.norm.a_2: min=1.0000, max=1.0000\n",
            "decoder.layers.3.sublayer.1.norm.b_2: min=0.0000, max=0.0000\n",
            "decoder.layers.3.sublayer.2.norm.a_2: min=1.0000, max=1.0000\n",
            "decoder.layers.3.sublayer.2.norm.b_2: min=0.0000, max=0.0000\n",
            "decoder.norm.a_2: min=1.0000, max=1.0000\n",
            "decoder.norm.b_2: min=0.0000, max=0.0000\n",
            "generator.proj.bias: min=-0.0625, max=0.0625\n",
            "\n",
            "================================================================================\n",
            "Epoch 1/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 10.1359 | Tokens/Sec: 5322 | Learning Rate: 0.00000025\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 9.3657 | Tokens/Sec: 26998 | Learning Rate: 0.00001211\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 8.7552 | Tokens/Sec: 28095 | Learning Rate: 0.00002446\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 8.3707 | Tokens/Sec: 27955 | Learning Rate: 0.00003681\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 7.7600 | Tokens/Sec: 26626 | Learning Rate: 0.00004916\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 7.0739 | Tokens/Sec: 27223 | Learning Rate: 0.00006152\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 7.1287 | Tokens/Sec: 26505 | Learning Rate: 0.00007387\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 7.0484 | Tokens/Sec: 26399 | Learning Rate: 0.00008622\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 6.9050 | Tokens/Sec: 25528 | Learning Rate: 0.00009857\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 6.8220 | Tokens/Sec: 26149 | Learning Rate: 0.00011093\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 6.7562 | Tokens/Sec: 26235 | Learning Rate: 0.00012328\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 6.2779 | Tokens/Sec: 25730 | Learning Rate: 0.00013563\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 6.5095 | Tokens/Sec: 26058 | Learning Rate: 0.00014798\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 6.4453 | Tokens/Sec: 27277 | Learning Rate: 0.00016034\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 5.8565 | Tokens/Sec: 27303 | Learning Rate: 0.00017269\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 6.1052 | Tokens/Sec: 28049 | Learning Rate: 0.00018504\n",
            "\n",
            "Epoch 1 完成:\n",
            "  Train Loss: 0.0070\n",
            "  Val Loss: 0.0060\n",
            "\n",
            "[VAL] Epoch 1 | Val Loss: 0.0060 | BLEU-4: 0.38\n",
            "Best BLEU so far: 0.00\n",
            "保存新最佳模型 (BLEU: 0.38)\n",
            "\n",
            "==================================================\n",
            "Epoch 1 翻译质量评估\n",
            "==================================================\n",
            "BLEU-4 Score: 0.38\n",
            "\n",
            "关键样本翻译:\n",
            "【我爱深度学习】 → The s\n",
            "【人工智能改变世界】 → The s\n",
            "【经济危机不断加深】 → The s\n",
            "\n",
            "================================================================================\n",
            "Epoch 2/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 6.4964 | Tokens/Sec: 26947 | Learning Rate: 0.00018652\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 6.1344 | Tokens/Sec: 28573 | Learning Rate: 0.00019888\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 6.0154 | Tokens/Sec: 28850 | Learning Rate: 0.00021123\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 6.2228 | Tokens/Sec: 28970 | Learning Rate: 0.00022358\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 6.1877 | Tokens/Sec: 28562 | Learning Rate: 0.00023594\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 5.7711 | Tokens/Sec: 28437 | Learning Rate: 0.00024829\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 5.7811 | Tokens/Sec: 28200 | Learning Rate: 0.00026064\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 5.6980 | Tokens/Sec: 28551 | Learning Rate: 0.00027299\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 5.9225 | Tokens/Sec: 28225 | Learning Rate: 0.00028535\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 5.7387 | Tokens/Sec: 28565 | Learning Rate: 0.00029770\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 5.5689 | Tokens/Sec: 28801 | Learning Rate: 0.00031005\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 5.3954 | Tokens/Sec: 28425 | Learning Rate: 0.00032240\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 5.2324 | Tokens/Sec: 28678 | Learning Rate: 0.00033476\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 5.3226 | Tokens/Sec: 28386 | Learning Rate: 0.00034711\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 5.5060 | Tokens/Sec: 28534 | Learning Rate: 0.00035946\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 5.4420 | Tokens/Sec: 27948 | Learning Rate: 0.00037181\n",
            "\n",
            "Epoch 2 完成:\n",
            "  Train Loss: 0.0055\n",
            "  Val Loss: 0.0051\n",
            "\n",
            "[VAL] Epoch 2 | Val Loss: 0.0051 | BLEU-4: 1.36\n",
            "Best BLEU so far: 0.38\n",
            "保存新最佳模型 (BLEU: 1.36)\n",
            "\n",
            "================================================================================\n",
            "Epoch 3/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 5.1589 | Tokens/Sec: 27792 | Learning Rate: 0.00037330\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 4.6233 | Tokens/Sec: 26667 | Learning Rate: 0.00038565\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 5.6075 | Tokens/Sec: 27336 | Learning Rate: 0.00039800\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 5.3398 | Tokens/Sec: 28231 | Learning Rate: 0.00041035\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 4.6737 | Tokens/Sec: 27970 | Learning Rate: 0.00042271\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 5.0361 | Tokens/Sec: 28060 | Learning Rate: 0.00043506\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 5.0435 | Tokens/Sec: 28139 | Learning Rate: 0.00044741\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 4.8457 | Tokens/Sec: 27218 | Learning Rate: 0.00045977\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 5.5513 | Tokens/Sec: 28464 | Learning Rate: 0.00047212\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 4.8977 | Tokens/Sec: 27078 | Learning Rate: 0.00048447\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 5.2967 | Tokens/Sec: 26610 | Learning Rate: 0.00049682\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 5.4029 | Tokens/Sec: 25660 | Learning Rate: 0.00050918\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 5.1930 | Tokens/Sec: 26782 | Learning Rate: 0.00052153\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 4.7785 | Tokens/Sec: 27206 | Learning Rate: 0.00053388\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 4.4956 | Tokens/Sec: 26238 | Learning Rate: 0.00054623\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 4.4914 | Tokens/Sec: 27052 | Learning Rate: 0.00055859\n",
            "\n",
            "Epoch 3 完成:\n",
            "  Train Loss: 0.0048\n",
            "  Val Loss: 0.0045\n",
            "\n",
            "[VAL] Epoch 3 | Val Loss: 0.0045 | BLEU-4: 1.00\n",
            "Best BLEU so far: 1.36\n",
            "\n",
            "==================================================\n",
            "Epoch 3 翻译质量评估\n",
            "==================================================\n",
            "BLEU-4 Score: 1.00\n",
            "\n",
            "关键样本翻译:\n",
            "【我爱深度学习】 → Is I\n",
            "【人工智能改变世界】 → A World\n",
            "【经济危机不断加深】 → Economic crisis\n",
            "\n",
            "================================================================================\n",
            "Epoch 4/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 4.7618 | Tokens/Sec: 28625 | Learning Rate: 0.00056007\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 4.6549 | Tokens/Sec: 28486 | Learning Rate: 0.00057242\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 4.5589 | Tokens/Sec: 28211 | Learning Rate: 0.00058477\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 3.9472 | Tokens/Sec: 28282 | Learning Rate: 0.00059713\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 4.7461 | Tokens/Sec: 28056 | Learning Rate: 0.00060948\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 4.2251 | Tokens/Sec: 28337 | Learning Rate: 0.00062183\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 4.8128 | Tokens/Sec: 27421 | Learning Rate: 0.00063418\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 4.3231 | Tokens/Sec: 25572 | Learning Rate: 0.00064654\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 4.5518 | Tokens/Sec: 26672 | Learning Rate: 0.00065889\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 3.9293 | Tokens/Sec: 25262 | Learning Rate: 0.00067124\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 4.5340 | Tokens/Sec: 27207 | Learning Rate: 0.00068360\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 4.2862 | Tokens/Sec: 27702 | Learning Rate: 0.00069595\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 4.4224 | Tokens/Sec: 27707 | Learning Rate: 0.00070830\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 4.1750 | Tokens/Sec: 26882 | Learning Rate: 0.00072065\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 4.4136 | Tokens/Sec: 26266 | Learning Rate: 0.00073301\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 4.1144 | Tokens/Sec: 26130 | Learning Rate: 0.00074536\n",
            "\n",
            "Epoch 4 完成:\n",
            "  Train Loss: 0.0043\n",
            "  Val Loss: 0.0041\n",
            "\n",
            "[VAL] Epoch 4 | Val Loss: 0.0041 | BLEU-4: 5.50\n",
            "Best BLEU so far: 1.36\n",
            "保存新最佳模型 (BLEU: 5.50)\n",
            "\n",
            "================================================================================\n",
            "Epoch 5/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 4.2474 | Tokens/Sec: 16285 | Learning Rate: 0.00074684\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 3.7333 | Tokens/Sec: 26608 | Learning Rate: 0.00075919\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 4.1024 | Tokens/Sec: 27420 | Learning Rate: 0.00077155\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 3.8353 | Tokens/Sec: 27631 | Learning Rate: 0.00078390\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 4.4113 | Tokens/Sec: 26556 | Learning Rate: 0.00079625\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 4.5776 | Tokens/Sec: 26787 | Learning Rate: 0.00080860\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 3.4886 | Tokens/Sec: 26052 | Learning Rate: 0.00082096\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 4.0581 | Tokens/Sec: 27937 | Learning Rate: 0.00083331\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 3.8722 | Tokens/Sec: 26952 | Learning Rate: 0.00084566\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 4.0338 | Tokens/Sec: 28351 | Learning Rate: 0.00085801\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 4.2910 | Tokens/Sec: 26385 | Learning Rate: 0.00087037\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 3.8517 | Tokens/Sec: 26757 | Learning Rate: 0.00088272\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 4.8885 | Tokens/Sec: 26991 | Learning Rate: 0.00089507\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 3.7649 | Tokens/Sec: 27842 | Learning Rate: 0.00090743\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 3.7742 | Tokens/Sec: 27532 | Learning Rate: 0.00091978\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 4.1894 | Tokens/Sec: 26377 | Learning Rate: 0.00093213\n",
            "\n",
            "Epoch 5 完成:\n",
            "  Train Loss: 0.0039\n",
            "  Val Loss: 0.0038\n",
            "\n",
            "[VAL] Epoch 5 | Val Loss: 0.0038 | BLEU-4: 1.59\n",
            "Best BLEU so far: 5.50\n",
            "\n",
            "==================================================\n",
            "Epoch 5 翻译质量评估\n",
            "==================================================\n",
            "BLEU-4 Score: 1.59\n",
            "\n",
            "关键样本翻译:\n",
            "【我爱深度学习】 → My Eit\n",
            "【人工智能改变世界】 → artificial\n",
            "【经济危机不断加深】 → Economic crisis\n",
            "\n",
            "================================================================================\n",
            "Epoch 6/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 4.0565 | Tokens/Sec: 25933 | Learning Rate: 0.00093361\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 4.0340 | Tokens/Sec: 27785 | Learning Rate: 0.00094597\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 4.1445 | Tokens/Sec: 27952 | Learning Rate: 0.00095832\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 3.5148 | Tokens/Sec: 28722 | Learning Rate: 0.00097067\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 3.7645 | Tokens/Sec: 28412 | Learning Rate: 0.00098302\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 3.6774 | Tokens/Sec: 28491 | Learning Rate: 0.00098465\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 3.7368 | Tokens/Sec: 28786 | Learning Rate: 0.00097860\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 4.2142 | Tokens/Sec: 29221 | Learning Rate: 0.00097265\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 3.7155 | Tokens/Sec: 28435 | Learning Rate: 0.00096682\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 3.8614 | Tokens/Sec: 28986 | Learning Rate: 0.00096108\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 3.6779 | Tokens/Sec: 28696 | Learning Rate: 0.00095545\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 3.4717 | Tokens/Sec: 28605 | Learning Rate: 0.00094992\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 3.4604 | Tokens/Sec: 27373 | Learning Rate: 0.00094448\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 4.6348 | Tokens/Sec: 27360 | Learning Rate: 0.00093913\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 3.6294 | Tokens/Sec: 28447 | Learning Rate: 0.00093388\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 4.0244 | Tokens/Sec: 25817 | Learning Rate: 0.00092871\n",
            "\n",
            "Epoch 6 完成:\n",
            "  Train Loss: 0.0037\n",
            "  Val Loss: 0.0036\n",
            "\n",
            "[VAL] Epoch 6 | Val Loss: 0.0036\n",
            "Best BLEU so far: 5.50\n",
            "\n",
            "================================================================================\n",
            "Epoch 7/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 3.2525 | Tokens/Sec: 29674 | Learning Rate: 0.00092809\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 3.8775 | Tokens/Sec: 26465 | Learning Rate: 0.00092302\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 3.3370 | Tokens/Sec: 26392 | Learning Rate: 0.00091803\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 3.5570 | Tokens/Sec: 27553 | Learning Rate: 0.00091311\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 3.6530 | Tokens/Sec: 27894 | Learning Rate: 0.00090828\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 3.8072 | Tokens/Sec: 28107 | Learning Rate: 0.00090352\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 3.2440 | Tokens/Sec: 28060 | Learning Rate: 0.00089884\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 3.3972 | Tokens/Sec: 28227 | Learning Rate: 0.00089423\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 3.9213 | Tokens/Sec: 27415 | Learning Rate: 0.00088969\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 3.9982 | Tokens/Sec: 27269 | Learning Rate: 0.00088521\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 4.3169 | Tokens/Sec: 28267 | Learning Rate: 0.00088081\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 3.5177 | Tokens/Sec: 28367 | Learning Rate: 0.00087646\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 3.5856 | Tokens/Sec: 27944 | Learning Rate: 0.00087219\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 3.5286 | Tokens/Sec: 27542 | Learning Rate: 0.00086797\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 3.6270 | Tokens/Sec: 27856 | Learning Rate: 0.00086382\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 3.0796 | Tokens/Sec: 27711 | Learning Rate: 0.00085972\n",
            "\n",
            "Epoch 7 完成:\n",
            "  Train Loss: 0.0035\n",
            "  Val Loss: 0.0035\n",
            "\n",
            "[VAL] Epoch 7 | Val Loss: 0.0035 | BLEU-4: 5.73\n",
            "Best BLEU so far: 5.50\n",
            "保存新最佳模型 (BLEU: 5.73)\n",
            "\n",
            "==================================================\n",
            "Epoch 7 翻译质量评估\n",
            "==================================================\n",
            "BLEU-4 Score: 5.73\n",
            "\n",
            "关键样本翻译:\n",
            "【我爱深度学习】 → I love I love\n",
            "【人工智能改变世界】 → AI in the World\n",
            "【经济危机不断加深】 → The Econom crisis deepens\n",
            "\n",
            "================================================================================\n",
            "Epoch 8/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 3.0398 | Tokens/Sec: 25776 | Learning Rate: 0.00085923\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 3.2707 | Tokens/Sec: 28307 | Learning Rate: 0.00085520\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 3.8939 | Tokens/Sec: 28367 | Learning Rate: 0.00085123\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 3.3929 | Tokens/Sec: 27600 | Learning Rate: 0.00084731\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 3.6005 | Tokens/Sec: 28046 | Learning Rate: 0.00084344\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 3.4308 | Tokens/Sec: 28492 | Learning Rate: 0.00083963\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 3.2354 | Tokens/Sec: 28151 | Learning Rate: 0.00083586\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 3.1416 | Tokens/Sec: 27115 | Learning Rate: 0.00083215\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 3.3339 | Tokens/Sec: 26502 | Learning Rate: 0.00082849\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 3.6122 | Tokens/Sec: 27613 | Learning Rate: 0.00082487\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 3.5368 | Tokens/Sec: 27676 | Learning Rate: 0.00082130\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 3.6959 | Tokens/Sec: 27026 | Learning Rate: 0.00081778\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 3.7386 | Tokens/Sec: 26066 | Learning Rate: 0.00081430\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 3.5721 | Tokens/Sec: 25797 | Learning Rate: 0.00081087\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 3.5497 | Tokens/Sec: 26076 | Learning Rate: 0.00080748\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 3.2119 | Tokens/Sec: 26691 | Learning Rate: 0.00080413\n",
            "\n",
            "Epoch 8 完成:\n",
            "  Train Loss: 0.0034\n",
            "  Val Loss: 0.0034\n",
            "\n",
            "[VAL] Epoch 8 | Val Loss: 0.0034 | BLEU-4: 2.27\n",
            "Best BLEU so far: 5.73\n",
            "\n",
            "================================================================================\n",
            "Epoch 9/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 3.4965 | Tokens/Sec: 25043 | Learning Rate: 0.00080373\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 3.5564 | Tokens/Sec: 28371 | Learning Rate: 0.00080043\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 3.7498 | Tokens/Sec: 29157 | Learning Rate: 0.00079717\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 3.5622 | Tokens/Sec: 27645 | Learning Rate: 0.00079394\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 3.2913 | Tokens/Sec: 26318 | Learning Rate: 0.00079076\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 3.4755 | Tokens/Sec: 25924 | Learning Rate: 0.00078761\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 3.3518 | Tokens/Sec: 25941 | Learning Rate: 0.00078451\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 3.5599 | Tokens/Sec: 26324 | Learning Rate: 0.00078143\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 3.4874 | Tokens/Sec: 26220 | Learning Rate: 0.00077840\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 3.0122 | Tokens/Sec: 26635 | Learning Rate: 0.00077540\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 3.3986 | Tokens/Sec: 27172 | Learning Rate: 0.00077243\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 3.9130 | Tokens/Sec: 26929 | Learning Rate: 0.00076950\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 3.9738 | Tokens/Sec: 26668 | Learning Rate: 0.00076660\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 3.4331 | Tokens/Sec: 27842 | Learning Rate: 0.00076373\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 3.4159 | Tokens/Sec: 28165 | Learning Rate: 0.00076089\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 3.3241 | Tokens/Sec: 28144 | Learning Rate: 0.00075809\n",
            "\n",
            "Epoch 9 完成:\n",
            "  Train Loss: 0.0033\n",
            "  Val Loss: 0.0033\n",
            "\n",
            "[VAL] Epoch 9 | Val Loss: 0.0033 | BLEU-4: 3.86\n",
            "Best BLEU so far: 5.73\n",
            "\n",
            "==================================================\n",
            "Epoch 9 翻译质量评估\n",
            "==================================================\n",
            "BLEU-4 Score: 3.86\n",
            "\n",
            "关键样本翻译:\n",
            "【我爱深度学习】 → I E Edward\n",
            "【人工智能改变世界】 → AI Bigs\n",
            "【经济危机不断加深】 → The Economic crisis deepens\n",
            "\n",
            "================================================================================\n",
            "Epoch 10/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 3.3691 | Tokens/Sec: 25568 | Learning Rate: 0.00075776\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 3.3399 | Tokens/Sec: 27503 | Learning Rate: 0.00075499\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 3.2660 | Tokens/Sec: 27615 | Learning Rate: 0.00075225\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 3.4634 | Tokens/Sec: 27648 | Learning Rate: 0.00074954\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 3.7069 | Tokens/Sec: 27318 | Learning Rate: 0.00074686\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 3.3811 | Tokens/Sec: 28267 | Learning Rate: 0.00074421\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 3.2449 | Tokens/Sec: 27623 | Learning Rate: 0.00074158\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 3.4012 | Tokens/Sec: 27689 | Learning Rate: 0.00073899\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 3.1582 | Tokens/Sec: 27585 | Learning Rate: 0.00073642\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 3.2598 | Tokens/Sec: 27625 | Learning Rate: 0.00073387\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 3.2232 | Tokens/Sec: 27603 | Learning Rate: 0.00073136\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 3.3163 | Tokens/Sec: 28202 | Learning Rate: 0.00072887\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 3.4387 | Tokens/Sec: 26196 | Learning Rate: 0.00072640\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 3.2874 | Tokens/Sec: 25583 | Learning Rate: 0.00072396\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 3.2989 | Tokens/Sec: 27138 | Learning Rate: 0.00072154\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 3.1923 | Tokens/Sec: 27534 | Learning Rate: 0.00071915\n",
            "\n",
            "Epoch 10 完成:\n",
            "  Train Loss: 0.0032\n",
            "  Val Loss: 0.0033\n",
            "\n",
            "[VAL] Epoch 10 | Val Loss: 0.0033 | BLEU-4: 7.28\n",
            "Best BLEU so far: 5.73\n",
            "保存新最佳模型 (BLEU: 7.28)\n",
            "\n",
            "================================================================================\n",
            "Epoch 11/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 3.4522 | Tokens/Sec: 25862 | Learning Rate: 0.00071887\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 3.2555 | Tokens/Sec: 28285 | Learning Rate: 0.00071650\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 3.3189 | Tokens/Sec: 28146 | Learning Rate: 0.00071416\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 3.4836 | Tokens/Sec: 28090 | Learning Rate: 0.00071184\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 3.2250 | Tokens/Sec: 27414 | Learning Rate: 0.00070954\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 3.5683 | Tokens/Sec: 26491 | Learning Rate: 0.00070727\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 3.7717 | Tokens/Sec: 26105 | Learning Rate: 0.00070501\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 3.2413 | Tokens/Sec: 26396 | Learning Rate: 0.00070278\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 3.0757 | Tokens/Sec: 25708 | Learning Rate: 0.00070057\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 3.2584 | Tokens/Sec: 26363 | Learning Rate: 0.00069838\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 3.5620 | Tokens/Sec: 26082 | Learning Rate: 0.00069621\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 3.2616 | Tokens/Sec: 26376 | Learning Rate: 0.00069406\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 3.1925 | Tokens/Sec: 27789 | Learning Rate: 0.00069193\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 3.4943 | Tokens/Sec: 27306 | Learning Rate: 0.00068982\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 3.0246 | Tokens/Sec: 26112 | Learning Rate: 0.00068773\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 3.3339 | Tokens/Sec: 25954 | Learning Rate: 0.00068565\n",
            "\n",
            "Epoch 11 完成:\n",
            "  Train Loss: 0.0032\n",
            "  Val Loss: 0.0033\n",
            "\n",
            "[VAL] Epoch 11 | Val Loss: 0.0033 | BLEU-4: 7.84\n",
            "Best BLEU so far: 7.28\n",
            "保存新最佳模型 (BLEU: 7.84)\n",
            "\n",
            "==================================================\n",
            "Epoch 11 翻译质量评估\n",
            "==================================================\n",
            "BLEU-4 Score: 7.84\n",
            "\n",
            "关键样本翻译:\n",
            "【我爱深度学习】 → I love I\n",
            "【人工智能改变世界】 → AI changing the World\n",
            "【经济危机不断加深】 → The Economic crisis deepens\n",
            "\n",
            "================================================================================\n",
            "Epoch 12/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 3.6918 | Tokens/Sec: 24444 | Learning Rate: 0.00068541\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 3.2543 | Tokens/Sec: 25825 | Learning Rate: 0.00068336\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 3.2185 | Tokens/Sec: 26098 | Learning Rate: 0.00068132\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 3.3385 | Tokens/Sec: 25683 | Learning Rate: 0.00067931\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 3.5196 | Tokens/Sec: 28101 | Learning Rate: 0.00067731\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 3.2745 | Tokens/Sec: 27890 | Learning Rate: 0.00067533\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 3.1415 | Tokens/Sec: 28076 | Learning Rate: 0.00067337\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 3.5410 | Tokens/Sec: 28284 | Learning Rate: 0.00067142\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 2.9457 | Tokens/Sec: 27825 | Learning Rate: 0.00066949\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 3.2004 | Tokens/Sec: 28036 | Learning Rate: 0.00066758\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 3.2734 | Tokens/Sec: 26414 | Learning Rate: 0.00066569\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 3.6592 | Tokens/Sec: 26144 | Learning Rate: 0.00066381\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 3.0586 | Tokens/Sec: 25901 | Learning Rate: 0.00066194\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 3.2892 | Tokens/Sec: 27399 | Learning Rate: 0.00066009\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 3.5502 | Tokens/Sec: 27401 | Learning Rate: 0.00065826\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 3.2744 | Tokens/Sec: 27197 | Learning Rate: 0.00065644\n",
            "\n",
            "Epoch 12 完成:\n",
            "  Train Loss: 0.0031\n",
            "  Val Loss: 0.0032\n",
            "\n",
            "[VAL] Epoch 12 | Val Loss: 0.0032 | BLEU-4: 7.38\n",
            "Best BLEU so far: 7.84\n",
            "\n",
            "================================================================================\n",
            "Epoch 13/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 3.0942 | Tokens/Sec: 28835 | Learning Rate: 0.00065622\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 2.9587 | Tokens/Sec: 26411 | Learning Rate: 0.00065442\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 3.5287 | Tokens/Sec: 25550 | Learning Rate: 0.00065264\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 3.2156 | Tokens/Sec: 26575 | Learning Rate: 0.00065087\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 3.3859 | Tokens/Sec: 26876 | Learning Rate: 0.00064911\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 3.1713 | Tokens/Sec: 27640 | Learning Rate: 0.00064736\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 3.4361 | Tokens/Sec: 27514 | Learning Rate: 0.00064563\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 3.5391 | Tokens/Sec: 27410 | Learning Rate: 0.00064392\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 3.1655 | Tokens/Sec: 27144 | Learning Rate: 0.00064222\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 3.1112 | Tokens/Sec: 25580 | Learning Rate: 0.00064053\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 3.0003 | Tokens/Sec: 25184 | Learning Rate: 0.00063885\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 3.0946 | Tokens/Sec: 25385 | Learning Rate: 0.00063719\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 4.5539 | Tokens/Sec: 25956 | Learning Rate: 0.00063554\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 3.0414 | Tokens/Sec: 26213 | Learning Rate: 0.00063391\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 3.5386 | Tokens/Sec: 26173 | Learning Rate: 0.00063228\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 3.3531 | Tokens/Sec: 26814 | Learning Rate: 0.00063067\n",
            "\n",
            "Epoch 13 完成:\n",
            "  Train Loss: 0.0031\n",
            "  Val Loss: 0.0032\n",
            "\n",
            "[VAL] Epoch 13 | Val Loss: 0.0032 | BLEU-4: 3.41\n",
            "Best BLEU so far: 7.84\n",
            "\n",
            "==================================================\n",
            "Epoch 13 翻译质量评估\n",
            "==================================================\n",
            "BLEU-4 Score: 3.41\n",
            "\n",
            "关键样本翻译:\n",
            "【我爱深度学习】 → I’s Eisch\n",
            "【人工智能改变世界】 → Artificial Intelligence\n",
            "【经济危机不断加深】 → The Economic Crisis deepens\n",
            "\n",
            "================================================================================\n",
            "Epoch 14/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 3.1486 | Tokens/Sec: 25315 | Learning Rate: 0.00063048\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 3.3653 | Tokens/Sec: 27845 | Learning Rate: 0.00062888\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 3.0944 | Tokens/Sec: 28270 | Learning Rate: 0.00062729\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 3.4804 | Tokens/Sec: 27948 | Learning Rate: 0.00062572\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 3.1770 | Tokens/Sec: 27138 | Learning Rate: 0.00062416\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 3.4274 | Tokens/Sec: 25869 | Learning Rate: 0.00062261\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 3.1766 | Tokens/Sec: 27732 | Learning Rate: 0.00062107\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 3.3553 | Tokens/Sec: 27187 | Learning Rate: 0.00061954\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 3.7917 | Tokens/Sec: 26972 | Learning Rate: 0.00061802\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 3.5970 | Tokens/Sec: 26660 | Learning Rate: 0.00061652\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 3.3035 | Tokens/Sec: 26870 | Learning Rate: 0.00061503\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 3.0157 | Tokens/Sec: 26474 | Learning Rate: 0.00061354\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 3.1810 | Tokens/Sec: 27689 | Learning Rate: 0.00061207\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 3.0466 | Tokens/Sec: 27882 | Learning Rate: 0.00061061\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 3.1875 | Tokens/Sec: 28113 | Learning Rate: 0.00060915\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 3.6971 | Tokens/Sec: 27757 | Learning Rate: 0.00060771\n",
            "\n",
            "Epoch 14 完成:\n",
            "  Train Loss: 0.0031\n",
            "  Val Loss: 0.0032\n",
            "\n",
            "[VAL] Epoch 14 | Val Loss: 0.0032 | BLEU-4: 10.05\n",
            "Best BLEU so far: 7.84\n",
            "保存新最佳模型 (BLEU: 10.05)\n",
            "\n",
            "================================================================================\n",
            "Epoch 15/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 2.7395 | Tokens/Sec: 29039 | Learning Rate: 0.00060754\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 3.3136 | Tokens/Sec: 28051 | Learning Rate: 0.00060611\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 3.3648 | Tokens/Sec: 28215 | Learning Rate: 0.00060469\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 3.2368 | Tokens/Sec: 28010 | Learning Rate: 0.00060328\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 3.1808 | Tokens/Sec: 28060 | Learning Rate: 0.00060188\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 3.2125 | Tokens/Sec: 28200 | Learning Rate: 0.00060049\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 2.8974 | Tokens/Sec: 27821 | Learning Rate: 0.00059911\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 3.0203 | Tokens/Sec: 28071 | Learning Rate: 0.00059774\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 3.3842 | Tokens/Sec: 27972 | Learning Rate: 0.00059638\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 2.7528 | Tokens/Sec: 27517 | Learning Rate: 0.00059502\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 3.3591 | Tokens/Sec: 28237 | Learning Rate: 0.00059368\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 4.0456 | Tokens/Sec: 28030 | Learning Rate: 0.00059234\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 3.7023 | Tokens/Sec: 27608 | Learning Rate: 0.00059102\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 3.1660 | Tokens/Sec: 26448 | Learning Rate: 0.00058970\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 3.2895 | Tokens/Sec: 25262 | Learning Rate: 0.00058839\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 3.3121 | Tokens/Sec: 26647 | Learning Rate: 0.00058709\n",
            "\n",
            "Epoch 15 完成:\n",
            "  Train Loss: 0.0031\n",
            "  Val Loss: 0.0032\n",
            "\n",
            "[VAL] Epoch 15 | Val Loss: 0.0032 | BLEU-4: 6.34\n",
            "Best BLEU so far: 10.05\n",
            "\n",
            "==================================================\n",
            "Epoch 15 翻译质量评估\n",
            "==================================================\n",
            "BLEU-4 Score: 6.34\n",
            "\n",
            "关键样本翻译:\n",
            "【我爱深度学习】 → My Deep learning\n",
            "【人工智能改变世界】 → Artificial intelligence Change the World\n",
            "【经济危机不断加深】 → Economic Crisis deepens\n",
            "\n",
            "================================================================================\n",
            "Epoch 16/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 3.1874 | Tokens/Sec: 22320 | Learning Rate: 0.00058694\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 2.7187 | Tokens/Sec: 26159 | Learning Rate: 0.00058565\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 3.5234 | Tokens/Sec: 26725 | Learning Rate: 0.00058437\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 2.9627 | Tokens/Sec: 25760 | Learning Rate: 0.00058309\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 2.9966 | Tokens/Sec: 27589 | Learning Rate: 0.00058183\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 3.7926 | Tokens/Sec: 28004 | Learning Rate: 0.00058057\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 3.3831 | Tokens/Sec: 28067 | Learning Rate: 0.00057932\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 3.2456 | Tokens/Sec: 26832 | Learning Rate: 0.00057808\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 3.5498 | Tokens/Sec: 25662 | Learning Rate: 0.00057685\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 3.0404 | Tokens/Sec: 25662 | Learning Rate: 0.00057563\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 3.1972 | Tokens/Sec: 25878 | Learning Rate: 0.00057441\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 3.4556 | Tokens/Sec: 26212 | Learning Rate: 0.00057320\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 3.5003 | Tokens/Sec: 25636 | Learning Rate: 0.00057200\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 3.0458 | Tokens/Sec: 26019 | Learning Rate: 0.00057081\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 3.3999 | Tokens/Sec: 24697 | Learning Rate: 0.00056962\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 3.1552 | Tokens/Sec: 25945 | Learning Rate: 0.00056844\n",
            "\n",
            "Epoch 16 完成:\n",
            "  Train Loss: 0.0030\n",
            "  Val Loss: 0.0032\n",
            "\n",
            "[VAL] Epoch 16 | Val Loss: 0.0032 | BLEU-4: 5.83\n",
            "Best BLEU so far: 10.05\n",
            "\n",
            "================================================================================\n",
            "Epoch 17/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 3.1563 | Tokens/Sec: 27185 | Learning Rate: 0.00056830\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 2.9596 | Tokens/Sec: 27421 | Learning Rate: 0.00056713\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 3.3892 | Tokens/Sec: 27826 | Learning Rate: 0.00056596\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 2.7708 | Tokens/Sec: 27546 | Learning Rate: 0.00056481\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 3.1235 | Tokens/Sec: 27111 | Learning Rate: 0.00056366\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 3.2756 | Tokens/Sec: 27329 | Learning Rate: 0.00056252\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 3.2902 | Tokens/Sec: 27120 | Learning Rate: 0.00056138\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 3.3500 | Tokens/Sec: 28516 | Learning Rate: 0.00056025\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 3.0450 | Tokens/Sec: 28232 | Learning Rate: 0.00055913\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 3.0831 | Tokens/Sec: 28040 | Learning Rate: 0.00055801\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 3.3737 | Tokens/Sec: 27570 | Learning Rate: 0.00055690\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 3.2924 | Tokens/Sec: 27707 | Learning Rate: 0.00055580\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 3.1950 | Tokens/Sec: 27478 | Learning Rate: 0.00055471\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 3.4406 | Tokens/Sec: 28506 | Learning Rate: 0.00055362\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 3.0318 | Tokens/Sec: 28694 | Learning Rate: 0.00055254\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 2.9208 | Tokens/Sec: 27500 | Learning Rate: 0.00055146\n",
            "\n",
            "Epoch 17 完成:\n",
            "  Train Loss: 0.0030\n",
            "  Val Loss: 0.0032\n",
            "\n",
            "[VAL] Epoch 17 | Val Loss: 0.0032 | BLEU-4: 9.17\n",
            "Best BLEU so far: 10.05\n",
            "\n",
            "==================================================\n",
            "Epoch 17 翻译质量评估\n",
            "==================================================\n",
            "BLEU-4 Score: 9.17\n",
            "\n",
            "关键样本翻译:\n",
            "【我爱深度学习】 → I loved Deep learning\n",
            "【人工智能改变世界】 → Artificial Intelligence changing the World\n",
            "【经济危机不断加深】 → The Economic Crisis deepens\n",
            "\n",
            "================================================================================\n",
            "Epoch 18/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 2.7837 | Tokens/Sec: 18390 | Learning Rate: 0.00055133\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 3.0176 | Tokens/Sec: 28395 | Learning Rate: 0.00055026\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 2.8835 | Tokens/Sec: 29053 | Learning Rate: 0.00054920\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 3.1757 | Tokens/Sec: 28824 | Learning Rate: 0.00054814\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 2.9693 | Tokens/Sec: 28323 | Learning Rate: 0.00054709\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 3.4197 | Tokens/Sec: 27981 | Learning Rate: 0.00054604\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 3.2990 | Tokens/Sec: 28187 | Learning Rate: 0.00054501\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 3.1885 | Tokens/Sec: 28104 | Learning Rate: 0.00054397\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 3.0954 | Tokens/Sec: 28444 | Learning Rate: 0.00054294\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 3.2336 | Tokens/Sec: 28321 | Learning Rate: 0.00054192\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 3.4169 | Tokens/Sec: 28126 | Learning Rate: 0.00054091\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 3.0975 | Tokens/Sec: 27656 | Learning Rate: 0.00053990\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 3.0454 | Tokens/Sec: 26560 | Learning Rate: 0.00053889\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 2.9080 | Tokens/Sec: 26716 | Learning Rate: 0.00053789\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 3.6180 | Tokens/Sec: 26195 | Learning Rate: 0.00053690\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 3.2379 | Tokens/Sec: 26589 | Learning Rate: 0.00053591\n",
            "\n",
            "Epoch 18 完成:\n",
            "  Train Loss: 0.0030\n",
            "  Val Loss: 0.0032\n",
            "\n",
            "[VAL] Epoch 18 | Val Loss: 0.0032 | BLEU-4: 4.52\n",
            "Best BLEU so far: 10.05\n",
            "\n",
            "================================================================================\n",
            "Epoch 19/30\n",
            "================================================================================\n",
            "Epoch Step: 1 | Accumulation Step: 0 | Loss: 2.8928 | Tokens/Sec: 28434 | Learning Rate: 0.00053580\n",
            "Epoch Step: 201 | Accumulation Step: 50 | Loss: 2.8136 | Tokens/Sec: 27269 | Learning Rate: 0.00053481\n",
            "Epoch Step: 401 | Accumulation Step: 100 | Loss: 3.0740 | Tokens/Sec: 26965 | Learning Rate: 0.00053384\n",
            "Epoch Step: 601 | Accumulation Step: 150 | Loss: 3.0898 | Tokens/Sec: 27652 | Learning Rate: 0.00053287\n",
            "Epoch Step: 801 | Accumulation Step: 200 | Loss: 2.9477 | Tokens/Sec: 27828 | Learning Rate: 0.00053190\n",
            "Epoch Step: 1001 | Accumulation Step: 250 | Loss: 1.6131 | Tokens/Sec: 25861 | Learning Rate: 0.00053094\n",
            "Epoch Step: 1201 | Accumulation Step: 300 | Loss: 3.0207 | Tokens/Sec: 26028 | Learning Rate: 0.00052998\n",
            "Epoch Step: 1401 | Accumulation Step: 350 | Loss: 3.0285 | Tokens/Sec: 26001 | Learning Rate: 0.00052903\n",
            "Epoch Step: 1601 | Accumulation Step: 400 | Loss: 3.2470 | Tokens/Sec: 26996 | Learning Rate: 0.00052809\n",
            "Epoch Step: 1801 | Accumulation Step: 450 | Loss: 3.2107 | Tokens/Sec: 25628 | Learning Rate: 0.00052715\n",
            "Epoch Step: 2001 | Accumulation Step: 500 | Loss: 2.8195 | Tokens/Sec: 26733 | Learning Rate: 0.00052621\n",
            "Epoch Step: 2201 | Accumulation Step: 550 | Loss: 3.1148 | Tokens/Sec: 27046 | Learning Rate: 0.00052528\n",
            "Epoch Step: 2401 | Accumulation Step: 600 | Loss: 3.1408 | Tokens/Sec: 26564 | Learning Rate: 0.00052436\n",
            "Epoch Step: 2601 | Accumulation Step: 650 | Loss: 3.2725 | Tokens/Sec: 27635 | Learning Rate: 0.00052344\n",
            "Epoch Step: 2801 | Accumulation Step: 700 | Loss: 3.2565 | Tokens/Sec: 27549 | Learning Rate: 0.00052252\n",
            "Epoch Step: 3001 | Accumulation Step: 750 | Loss: 3.7548 | Tokens/Sec: 28222 | Learning Rate: 0.00052161\n",
            "\n",
            "Epoch 19 完成:\n",
            "  Train Loss: 0.0030\n",
            "  Val Loss: 0.0031\n",
            "\n",
            "[VAL] Epoch 19 | Val Loss: 0.0031 | BLEU-4: 8.88\n",
            "Best BLEU so far: 10.05\n",
            "\n",
            "早停触发! 最佳BLEU: 10.05 @ epoch 19\n"
          ]
        }
      ],
      "source": [
        "# 启动训练\n",
        "train_state = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    criterion=criterion,\n",
        "    num_epochs=30,\n",
        "    accum_iter=config['accumulation_steps'],\n",
        "    device=device,\n",
        "    PAD=PAD,\n",
        "    val_monitor=val_monitor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL2_PxYJ__Pt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "加载的模型来自 Epoch 14\n",
            "训练损失: 0.0031\n",
            "验证损失: 0.0032\n",
            "输入: 我爱深度学习\n",
            "final_model输出: I love Deep learning\n",
            "earlystop_model输出: I love Deep learning\n",
            "\n",
            "输入: 人工智能改变世界\n",
            "final_model输出: artificial intelligence Change the World\n",
            "earlystop_model输出: AI Change the World\n",
            "\n",
            "输入: 今天天气真不错\n",
            "final_model输出: Today’s weather Help\n",
            "earlystop_model输出: Today’s weather True\n",
            "\n",
            "输入: 经济危机不断加深\n",
            "final_model输出: The Economic Crisis continues to deepen\n",
            "earlystop_model输出: Economic Crisis deepens\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 翻译推理\n",
        "checkpoint = torch.load('best_model.pt', weights_only=False, map_location=device)\n",
        "print(f\"加载的模型来自 Epoch {checkpoint['epoch']}\")\n",
        "print(f\"训练损失: {checkpoint['train_loss']:.4f}\")\n",
        "print(f\"验证损失: {checkpoint['val_loss']:.4f}\")\n",
        "\n",
        "test_model = Transformer(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    d_model=config['d_model'],\n",
        "    d_ff=config['d_ff'],\n",
        "    N=config['N'],\n",
        "    h=config['h'],\n",
        "    dropout=config['dropout']\n",
        ").to(device)\n",
        "\n",
        "test_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "test_model.eval()\n",
        "\n",
        "test_sentences = [\n",
        "    \"我爱深度学习\",\n",
        "    \"人工智能改变世界\", \n",
        "    \"今天天气真不错\",\n",
        "    \"经济危机不断加深\"\n",
        "]\n",
        "\n",
        "for sent in test_sentences:\n",
        "    # 假设有final_model（内存里的最后的model）\n",
        "    final_translation = translate(model, sent, sp, device)\n",
        "    best_translation = translate(test_model, sent, sp, device)\n",
        "\n",
        "    print(f\"输入: {sent}\")\n",
        "    print(f\"final_model输出: {final_translation}\")\n",
        "    print(f\"earlystop_model输出: {best_translation}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env2-py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "018c739bc15f48ceabb6baf3c89a0471": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05991c9d98df40279900904568043a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_018c739bc15f48ceabb6baf3c89a0471",
            "placeholder": "​",
            "style": "IPY_MODEL_a01a7e3e293e44f28e7b0df8e15c667d",
            "value": " 11.3k/? [00:00&lt;00:00, 372kB/s]"
          }
        },
        "2b9d6ba9959048afa09ee9e3ee725549": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92c750d6b4da47a1ae4aada8f4a1bde8",
            "placeholder": "​",
            "style": "IPY_MODEL_369a3407998f4d9ca1418dfda31041cc",
            "value": "README.md: "
          }
        },
        "369a3407998f4d9ca1418dfda31041cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7910ee144af24a0daea7f552a1e1678a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b9d6ba9959048afa09ee9e3ee725549",
              "IPY_MODEL_ee5fd467a18c4fa2870d3950883e56b8",
              "IPY_MODEL_05991c9d98df40279900904568043a82"
            ],
            "layout": "IPY_MODEL_e3fb3b7a71a049029f99dfdd547b9567"
          }
        },
        "81c4e90790364ad68276ad7d5756e350": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "92c750d6b4da47a1ae4aada8f4a1bde8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a01a7e3e293e44f28e7b0df8e15c667d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3fb3b7a71a049029f99dfdd547b9567": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee5fd467a18c4fa2870d3950883e56b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f015f6c4f1a34c38aec90330cae84447",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81c4e90790364ad68276ad7d5756e350",
            "value": 1
          }
        },
        "f015f6c4f1a34c38aec90330cae84447": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
